[{"content":"Meta-Analysis Imagine you’re studying the effects of non-nutritive sweeteners on blood pressure. You’re curious about whether they eventually increase, decrease, or have no impact on blood pressure. You review various studies and find quite conflicting results. Some studies suggest that consuming non-nutritive sweeteners can increase blood pressure, while others show quite opposite results. Also, some studies indicate no effect on blood pressue from consuming non-nutritive sweeteners. This situation presents a challenge that you may address by conducting a meta-analysis. Meta-analysis pools data from multiple studies to provide a more precise estimate based on existing evidence. Seems quite intriguing, doesn’t it?\nBefore getting into meta-analysis itself, let\u0026rsquo;s have a better understanding of different types of studies:\nPrimary Studies These types of studies are conducted on humans, animals, and so on. In technical terminology, they’re referred to as original studies.\nSecondary Studies These types of studies are conducted on the findings of primary studies, including:\nNarrative reviews: They can be conducted with as few as two studies. they may not have a specific objective and might cover a broad topic, for example: Efficacy of vitamin E supplementation on routine diseases in women. Also, they may not include every article available within the topic area under review. Systematic reviews: They have a defined objective, for example: Efficacy of vitamin E supplementation on blood pressure in adult women. Also, all articles within the scope of the topic under review must be included. Meta-analyses: Similar in every respect to a systematic review, with the difference that statistical analysis is also performed. Meta-analysis is essentially a re-analysis of the findings from primary studies. In a meta-analysis, the quantitative data from primary studies (such as means, odds ratios, etc.) are combined into a single result (for example, one mean, one odds ratio, etc.) Therefore, drawing the final conclusion in a meta-analysis is easier. An example would be: Effect of vitamin E supplementation on CRP concentrations in adults: A meta-analysis Meta-analyses sit at the top of the evidence pyramid So you may ask why meta-anlysis? In the biomedical sciences field, there’s a hierarchical evidence pyramid that outlines the significance of different studies conducted in this area. Interestingly, meta-analysis studies, which combine existing studies, are placed at the top of this pyramid, highlighting their importance.\nSource: The Logic of Science\nIn the initial step, we should have a better understanding of different types of RCTs. We want to know how we can differentiate them and how we should treat each study’s data based on its type.\nStudy Designs Randomized Clinical Trial (RCT): Participants are randomly assigned to intervention and control groups. Single-blind Trial: Participants are unaware of the intervention details. Double-blind Trial: Both participants and researchers are blind to assignments. Triple-blind Trial: Participants, researchers, and analysts are blind to group assignments. Semi-experimental (Quasi-experimental) Study: Lacks randomization. Open Trial: No control group; only an intervention group. Community Trial: Interventions at the community or group level. For example, iron fortification of flour or vitamin D supplementation programs implemented across schools. Choosing a Topic for Meta-Analysis To select a topic, we should consider:\nPrinciple 1) Type of study: Clinical trial vs. observational study. We focus on clinical trials in this series.\nPrinciple 2) Study design: RCTs are preferred for fewer biases.\nPrinciple 3) Type of intervention: e.g., supplementation, drug therapy, dietary intervention, educational intervention, etc.\nPrinciple 4) Target population: Children vs. adults, patients vs. healthy individuals.\nPrinciple 5) Dependent variables: The dependent variables to be considered: inflammatory markers, lipid profile, weight, etc.\nPrinciple 6) Intervention duration: The minimum duration of intervention: one day, one week, two weeks, etc.\nPrinciple 7) Number of studies: To perform a meta-analysis, there must be at least three articles (in accordance with Principles 1–6) in the topic area under review. These three articles must provide the necessary data to carry out a meta-analysis on a single dependent variable.\nRequired data for interventional meta-analysis:\nOption 1: The mean and standard deviation (SD) of the dependent variable before and after the intervention, reported separately for the intervention and control groups.\nOption 2: The mean change (i.e., difference between pre- and post-intervention) and its SD for the dependent variable, again separately for the intervention and control groups.\nNote: Some studies report SE (standard error), IQR (interquartile range), 95% CI (confidence interval), or range. These can all be converted to SD for pooling.\nPrinciple 8) Is there an existing meta-analysis on our topic? If a meta-analysis on our topic has already been published, we should usually drop the topic—since it no longer has publishable value—unless:\nThe published meta-analysis contains methodological flaws. It is possible to update the meta-analysis due to several new studies published in that field since then. Well, that seems like a lot for now! In Part 2, we will look at how we should prepare our dataset for performing meta-analyses.\nStay tuned!\nReference: For guidelines and detailed methodology, see the Cochrane Handbook by Julian P. T. Higgins and Sally Green.\n","date":"2025-05-16T09:00:00-08:00","permalink":"http://localhost:5082/csc-data-blog/p/meta-analysis-on-randomized-clinical-trials-part-1/","title":"Meta-Analysis on Randomized Clinical Trials - Part 1"},{"content":" Definitions API\nAn application programming interface (API) is a formalized way of interacting with another piece of hardware or software without needing to know how that hardware or software actually works. As such, it is a convenient method of making requests to get, update, or push information between your system and the remote system. As a loose analogy, you can think of this as a menu in a resuraunt; you don\u0026rsquo;t need to know how the kitchen makes your food, but you need to know what you can ask for from your server. When you want scrambled eggs, you don\u0026rsquo;t need to go into the kitchen and make these from scratch, you simply put in a request based on the options on the menu.\nJSON\nJavaScript Object Notation (JSON) is a method for storing and transmitting data primarily using key value pairs. It is compact, flexible, human readable, and works well in a Web based environment. It is generally the default format in which data is transmitted from an API.\nSet up The Abacus API is accessible through http, which means we can access the API using any http compliant tool, including a web browser.\nBy default, the API returns data in JSON notation. One of the issues we have when working in R is that R excels with rectangular data, especially if working in the Tidyverse. So we need to wrangle the JSON formatted data into a data structure more amenable to R. This begins by moving content into a list after which we can transition it to a dataframe, or tibble, as needed.\nTo do this all, we\u0026rsquo;ll use the following four libraries, although, as we\u0026rsquo;ll see later, we can get away with just the httr2 package:\n1 2 library(httr2) # http protocols library(jsonlite) # json to r data structures Steps In the proceding sections we\u0026rsquo;ll cover the following steps:\nUsing the search API to look for relevant datasets. Plugging a unique ID associated with the dataset(s) of interest into the datasets api to identify the files of interest. This involves first identifying a new unique ID for each dataset, only accessible through the datasets API, and the version of the dataset we\u0026rsquo;re interested in (an Abacus record is version controlled). We will then use the urls associated with the data files to download the relevant data. We will be working with the Statistics Canada Labour Force Surveys for this exercise.\nCaveats This is a simplified overview of API interactions. So, for example, we will not be doing any error handling, nor will we deal with using authentication tokens. But these are easily built in once the general approach is understood, and the httr2 package has functions for handling these things.\nBuilding the Connection Abacus is built on a Dataverse instance. You can refer to the Dataverse API documentation for more information on interacting with the API.\nAbacus has more than one API. This is common when interacting with APIs. Each API gives you access to certain functionalities and returns certain pieces of data. Here, we\u0026rsquo;ll review the search and datasets APIs; the former to locate content, the latter to retrieve that content. We will ultimately use the access API with download.file() to acquire the data.\nIn either case, we start with the base url into the API, which we\u0026rsquo;ll store in a variable.\n1 base_url \u0026lt;- \u0026#34;https://abacus.library.ubc.ca/api/\u0026#34; Next, we can add on the specific API we intend to search.\n1 search_api \u0026lt;- \u0026#34;search?q=\u0026#34; The search API requires a query parameter, ie, something to search for. This is articulated as a string, following the ?q=.\nNote: When using the httr2 package, if the query string is multi-word, spaces need to be filled with %20. For example, a search for labour force will fail, and need to be replaced with labour%20force.\nSearch API There are a variety of fields and file types that we can search through the search API, noting that, as a data repository, objects in Abacus are organized into dataverses, datasets, and files, where datasets reside in dataverses, and files within datasets. Datasets provide a high level overview of the data they hold, including descriptions, number of files, who produced the files, when they were uploaded, etc.\nWe\u0026rsquo;ve already built our base_url and added on the requisite text to access the search API, search_api. We\u0026rsquo;ll now build our query, specifying the query and the file type of interest, ie, do we want to search for a dataverse, dataset, or file.\n1 2 search_query \u0026lt;- \u0026#34;title:labour\u0026#34; # retrieve everything with the word labour in the title search_type \u0026lt;- \u0026#34;\u0026amp;type=dataset\u0026#34; # limit to datasets We can then paste these together to construct a full query.\n1 (search_api_call \u0026lt;- paste0(base_url, search_api, search_query, search_type)) 1 ## [1] \u0026#34;https://abacus.library.ubc.ca/api/search?q=title:labour\u0026amp;type=dataset\u0026#34; To get a sense of what this looks like, we can load this query in our browser.\nImplementing the Search in R The hhtr2 package will allow us to make http requests from within R. The steps involved including:\na) building a request; and then b) performing that request, which results in some data being sent back to us.\nThese two steps are done, respectively, with the request() and req_perform() functions. We feed our search_api_call into the request() and then feed the result into the req_preform.\n1 2 3 search_resp \u0026lt;- search_api_call |\u0026gt; httr2::request() |\u0026gt; httr2::req_perform() What we get back is the same as what we saw in our browser, formatted as a list.\n1 str(search_resp) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 ## List of 7 ## $ method : chr \u0026#34;GET\u0026#34; ## $ url : chr \u0026#34;https://abacus.library.ubc.ca/api/search?q=title:labour\u0026amp;type=dataset\u0026#34; ## $ status_code: int 200 ## $ headers :List of 8 ## ..$ Date : chr \u0026#34;Tue, 08 Apr 2025 17:05:46 GMT\u0026#34; ## ..$ Server : chr \u0026#34;Apache/2.4.37 (Red Hat Enterprise Linux) OpenSSL/1.1.1k mod_R/1.2.9 R/3.5.0 mod_apreq2-20101207/2.8.1\u0026#34; ## ..$ X-Frame-Options : chr \u0026#34;SAMEORIGIN\u0026#34; ## ..$ Access-Control-Allow-Origin : chr \u0026#34;*\u0026#34; ## ..$ Access-Control-Allow-Methods: chr \u0026#34;PUT, GET, POST, DELETE, OPTIONS\u0026#34; ## ..$ Access-Control-Allow-Headers: chr \u0026#34;Accept, Content-Type, X-Dataverse-Key\u0026#34; ## ..$ Content-Type : chr \u0026#34;application/json;charset=UTF-8\u0026#34; ## ..$ Transfer-Encoding : chr \u0026#34;chunked\u0026#34; ## ..- attr(*, \u0026#34;class\u0026#34;)= chr \u0026#34;httr2_headers\u0026#34; ## $ body : raw [1:24063] 7b 22 73 74 ... ## $ request :List of 7 ## ..$ url : chr \u0026#34;https://abacus.library.ubc.ca/api/search?q=title:labour\u0026amp;type=dataset\u0026#34; ## ..$ method : NULL ## ..$ headers : list() ## ..$ body : NULL ## ..$ fields : list() ## ..$ options : list() ## ..$ policies: list() ## ..- attr(*, \u0026#34;class\u0026#34;)= chr \u0026#34;httr2_request\u0026#34; ## $ cache :\u0026lt;environment: 0x136290320\u0026gt; ## - attr(*, \u0026#34;class\u0026#34;)= chr \u0026#34;httr2_response\u0026#34; What we\u0026rsquo;re most interested in here is the body. However, the body is sent to us structured as json and in raw bytes. This is generally the case with an API response. httr2 has a built in function for converting the body from raw to something R can work with \u0026ndash; resp_body_json() \u0026ndash; and we\u0026rsquo;ll use this later, but we\u0026rsquo;ll start with the jsonlite package to explore this process step by step.\n1 summary(search_resp$body) 1 2 ## Length Class Mode ## 24063 raw raw Since we only want the body going forward, we\u0026rsquo;ll isolate this.\n1 search_body \u0026lt;- search_resp$body Data Wrangling Our first task is to convert this raw data to character data. We can do this with rawToChar.\n1 search_body_char \u0026lt;- rawToChar(search_body) We\u0026rsquo;re presented with a vector of length one, which if we print out the first bit of, does not look very friendly.\n1 summary(search_body_char) 1 2 ## Length Class Mode ## 1 character character 1 str(search_body_char) 1 ## chr \u0026#34;{\\\u0026#34;status\\\u0026#34;:\\\u0026#34;OK\\\u0026#34;,\\\u0026#34;data\\\u0026#34;:{\\\u0026#34;q\\\u0026#34;:\\\u0026#34;title:labour\\\u0026#34;,\\\u0026#34;total_count\\\u0026#34;:111,\\\u0026#34;start\\\u0026#34;:0,\\\u0026#34;spelling_alternatives\\\u0026#34;:{\u0026#34;| __truncated__ This looks a little friendlier with cat()\n1 2 # not executed cat(search_body_char) Our next task then, is to convert this JSON notation to a suitable R data structure, in this case, a list. We\u0026rsquo;ll do this with the jsonlite package.\n1 search_body_list \u0026lt;- jsonlite::fromJSON(search_body_char) If we look at how this has been converted, we can see it\u0026rsquo;s a list of two.\n1 summary(search_body_list) 1 2 3 ## Length Class Mode ## status 1 -none- character ## data 6 -none- list The data list is what we want. We\u0026rsquo;ll pull that out and then explore that list.\n1 2 search_data \u0026lt;- search_body_list$data summary(search_data) Length Class Mode q 1 -none- character total_count 1 -none- numeric start 1 -none- numeric spelling_alternatives 0 -none- list items 27 data.frame list count_in_response 1 -none- numeric We\u0026rsquo;re slowly whittling this down. This now looks almost identical to what we pulled up in our browser earlier. The actual relevant content is in the items list. But we\u0026rsquo;ll also need the total_count for when we need to request all the data from the server in a loop. For now, let\u0026rsquo;s look at the 26 items in there.\n1 names(search_data$items) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## [1] \u0026#34;name\u0026#34; \u0026#34;type\u0026#34; ## [3] \u0026#34;url\u0026#34; \u0026#34;global_id\u0026#34; ## [5] \u0026#34;description\u0026#34; \u0026#34;published_at\u0026#34; ## [7] \u0026#34;publisher\u0026#34; \u0026#34;citationHtml\u0026#34; ## [9] \u0026#34;identifier_of_dataverse\u0026#34; \u0026#34;name_of_dataverse\u0026#34; ## [11] \u0026#34;citation\u0026#34; \u0026#34;storageIdentifier\u0026#34; ## [13] \u0026#34;keywords\u0026#34; \u0026#34;subjects\u0026#34; ## [15] \u0026#34;fileCount\u0026#34; \u0026#34;versionId\u0026#34; ## [17] \u0026#34;versionState\u0026#34; \u0026#34;majorVersion\u0026#34; ## [19] \u0026#34;minorVersion\u0026#34; \u0026#34;createdAt\u0026#34; ## [21] \u0026#34;updatedAt\u0026#34; \u0026#34;contacts\u0026#34; ## [23] \u0026#34;publications\u0026#34; \u0026#34;producers\u0026#34; ## [25] \u0026#34;geographicCoverage\u0026#34; \u0026#34;authors\u0026#34; ## [27] \u0026#34;relatedMaterial\u0026#34; We don\u0026rsquo;t need all this data. We should be able to do some evaluation with the name, description, fileCount, producers, published_at, url, and, for later retrieval, global_id. Let\u0026rsquo;s look at these quickly, noting importantly, that producers is another list, that we\u0026rsquo;ll need to wrangle some how.\n1 2 vars_of_interest \u0026lt;- c(\u0026#34;name\u0026#34;, \u0026#34;description\u0026#34;, \u0026#34;fileCount\u0026#34;, \u0026#34;producers\u0026#34;, \u0026#34;published_at\u0026#34;, \u0026#34;url\u0026#34;, \u0026#34;global_id\u0026#34;) lapply(search_data$items[vars_of_interest], class) # show the class of each list item 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ## $name ## [1] \u0026#34;character\u0026#34; ## ## $description ## [1] \u0026#34;character\u0026#34; ## ## $fileCount ## [1] \u0026#34;integer\u0026#34; ## ## $producers ## [1] \u0026#34;list\u0026#34; ## ## $published_at ## [1] \u0026#34;character\u0026#34; ## ## $url ## [1] \u0026#34;character\u0026#34; ## ## $global_id ## [1] \u0026#34;character\u0026#34; Getting All the Data At this stage, we have what we need to pull search results from Abacus. You\u0026rsquo;ll note that the search returned a result set of 109, but only 10 items:\n1 (search_body_list$data$total_count) 1 ## [1] 111 1 (nrow(search_body_list$data$items)) 1 ## [1] 10 Going forward, we need to paginate through the result set to retrieve all the items.\nA few things to note:\nBy default, the search API returns 10 results per request. It allows for a maximum of 1000 per request. This is handled in the per_page parameter. There is a start parameter that we can feed into the request; this indicates which record to start retrieving data from. This number will need to be updated as we iterate over our requests, for example, if in the first call we pull records 1:1000, in the second call, we need to start at 1001. This is handled in the start parameter. R starts counting at 1, other languages start at 0, so the first record returned by the API is indexed at 0 on the server, not 1. APIs will have a limit on the number of requests you can make in any given time span. We\u0026rsquo;ll build in a simple delay using Sys.Sleep(), but httr2 has a built in function for handling this as well. Below is a function with the above steps built into it. It takes two arguments, a search string and the number of results to retrieve per request. It also prints some progress information to the console.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 search_datasets \u0026lt;- function(query, per_page) { # Build the query url \u0026lt;- paste0(\u0026#34;https://abacus.library.ubc.ca/api/search?q=\u0026#34;, query, \u0026#34;\u0026amp;type=dataset\u0026#34;) # Initial call to figure out total records resp \u0026lt;- url |\u0026gt; httr2::request() |\u0026gt; httr2::req_perform() resp_body \u0026lt;- jsonlite::fromJSON(rawToChar(resp$body)) total_count \u0026lt;- resp_body$data$total_count # Give some feedback cat(paste0(\u0026#34;There are \u0026#34;, total_count, \u0026#34; records to be fetched.\\n\u0026#34;, \u0026#34;Fetching \u0026#34;, per_page, \u0026#34; records per call.\\nThis will require \u0026#34;, ceiling(total_count/per_page), \u0026#34; calls.\\n\\n\u0026#34;)) # build place holders for loop name \u0026lt;- vector() description \u0026lt;- vector() file_count \u0026lt;- vector() producers \u0026lt;- list() pub_date \u0026lt;- vector() global_id \u0026lt;- vector() handle \u0026lt;- vector() # establish the starting point start_point \u0026lt;- 0 # create a counter for tracking calls call_counter \u0026lt;- 1 # run the loop while(length(handle) \u0026lt; total_count) { # while the number of retrieved records is \u0026lt; the total count cat(paste0(\u0026#34;Call \u0026#34;, call_counter, \u0026#34;\\n\u0026#34;)) # Indicate what call we\u0026#39;re on to the API req \u0026lt;- httr2::request(paste0(url, \u0026#34;\u0026amp;start=\u0026#34;, start_point, \u0026#34;\u0026amp;per_page=\u0026#34;, per_page)) # create the request resp \u0026lt;- httr2::req_perform(req) # perform the request resp_body \u0026lt;- jsonlite::fromJSON(rawToChar(resp$body)) # convert the body from raw JSON to a list resp_body_name \u0026lt;- resp_body$data$items$name # get the name resp_body_desc \u0026lt;- resp_body$data$items$description # get the description resp_body_file_count \u0026lt;- resp_body$data$items$fileCount # get the file count resp_body_producers \u0026lt;- resp_body$data$items$producers # get the producers resp_body_handle \u0026lt;- resp_body$data$items$url #get the url resp_body_global_id \u0026lt;- resp_body$data$items$global_id # get id resp_body_pubdate \u0026lt;- resp_body$data$items$published_at # get update date # update the place holders: name \u0026lt;- c(name, resp_body_name) description \u0026lt;- c(description, resp_body_desc) file_count \u0026lt;- c(file_count, resp_body_file_count) producers \u0026lt;- c(producers, resp_body_producers) pub_date \u0026lt;- c(pub_date, resp_body_pubdate) handle \u0026lt;- c(handle, resp_body_handle) global_id \u0026lt;- c(global_id, resp_body_global_id) # update counters start_point \u0026lt;- start_point + per_page # increment the start_point call_counter \u0026lt;- call_counter + 1 # increment the counter Sys.sleep(.1) # pause before making a new call } # When all is said and done, compile the place holders into a list and return this object return(list(\u0026#34;name\u0026#34; = name, \u0026#34;description\u0026#34; = description, \u0026#34;file_count\u0026#34; = file_count, \u0026#34;producers\u0026#34; = producers, \u0026#34;pub_date\u0026#34; = pub_date, \u0026#34;handle\u0026#34; = handle, \u0026#34;global_id\u0026#34; = global_id)) } We can then call that function, here adding an additional parameter indicating that we only want to search the title and that we want 50 results per query. We\u0026rsquo;re being specifically broad with our search.\n1 labour \u0026lt;- search_datasets(\u0026#34;title:labour\u0026#34;, 50) 1 2 3 4 5 6 7 ## There are 111 records to be fetched. ## Fetching 50 records per call. ## This will require 3 calls. ## ## Call 1 ## Call 2 ## Call 3 We now have record data that we can explore.\n1 summary(labour) Length Class Mode name 111 -none- character description 111 -none- character file_count 111 -none- numeric producers 111 -none- list pub_date 111 -none- character handle 111 -none- character global_id 111 -none- character We can make this a bit easier for ourselves by converting this into a dataframe or tibble. There are several ways to approach this, considering that producers is a list \u0026ndash; it is a list as some records have more than one producer associated them. Here, I\u0026rsquo;ll treat datasets with multiple producers as a different record authority than if one of these co-producers produced a standalone dataset. To do this, we\u0026rsquo;ll first collapse the producers nested lists, and then flatten the result into a single list.\n1 2 3 4 5 labour$producers \u0026lt;- labour$producers |\u0026gt; lapply(function(x) paste(x, collapse = \u0026#34;, \u0026#34;)) |\u0026gt; unlist() head(labour$producers, n = 20) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ## [1] \u0026#34;Statistics Canada\u0026#34; ## [2] \u0026#34;Statistics Canada\u0026#34; ## [3] \u0026#34;Statistics Canada\u0026#34; ## [4] \u0026#34;Statistics Canada\u0026#34; ## [5] \u0026#34;Statistics Canada\u0026#34; ## [6] \u0026#34;Statistics Canada\u0026#34; ## [7] \u0026#34;Statistics Canada\u0026#34; ## [8] \u0026#34;Statistics Canada. Labour Force Survey Division\u0026#34; ## [9] \u0026#34;Statistics Canada. Labour Force Survey Division\u0026#34; ## [10] \u0026#34;Statistics Canada\u0026#34; ## [11] \u0026#34;Statistics Canada\u0026#34; ## [12] \u0026#34;Statistics Canada\u0026#34; ## [13] \u0026#34;Statistics Canada\u0026#34; ## [14] \u0026#34;Statistics Canada\u0026#34; ## [15] \u0026#34;Statistics Canada\u0026#34; ## [16] \u0026#34;Statistics Canada\u0026#34; ## [17] \u0026#34;Statistics Canada\u0026#34; ## [18] \u0026#34;Statistics Canada\u0026#34; ## [19] \u0026#34;Statistics Canada\u0026#34; ## [20] \u0026#34;Statistics Canada\u0026#34; We can easily convert this to a dataframe now.\n1 2 labour \u0026lt;- as.data.frame(labour) head(labour) name description file_count producers pub_date handle global_id Labour Force Survey, 2021 LFS data are used to produce the well-known ... 91 Statistics Canada 2023-03-16T22:00:56Z https://hdl.handle.net/11272.1/AB2/HP9TEK hdl:11272.1/AB2/HP9TEK Labour Force Survey, 2022 LFS data are used to produce the well-known ... 57 Statistics Canada 2023-03-16T22:01:02Z https://hdl.handle.net/11272.1/AB2/SRAU7E hdl:11272.1/AB2/SRAU7E Labour Force Survey, 2023 LFS data are used to produce the well-known ... 31 Statistics Canada 2024-01-05T17:38:25Z https://hdl.handle.net/11272.1/AB2/IJU1QK hdl:11272.1/AB2/IJU1QK Labour Force Survey, 2006 The Labour Force Survey provides estimates o... 26 Statistics Canada 2023-03-16T21:47:04Z https://hdl.handle.net/11272.1/AB2/0B5LPL hdl:11272.1/AB2/0B5LPL Labour Force Survey, 2008 The Labour Force Survey provides estimates o... 26 Statistics Canada 2023-03-16T21:47:20Z https://hdl.handle.net/11272.1/AB2/LA3WXI hdl:11272.1/AB2/LA3WXI Labour Force Survey, 2011 The Labour Force Survey provides estimates o... 26 Statistics Canada 2023-03-16T21:47:47Z https://hdl.handle.net/11272.1/AB2/GK3SFF hdl:11272.1/AB2/GK3SFF Explore We have several datasets with labour in the title. One way of handling exploring this is to strip the dates so that we can look for unique values. First we extract the date:\n1 2 labour$year \u0026lt;- gsub(\u0026#34;.*?([0-9]+).*$\u0026#34;, \u0026#34;\\\\1\u0026#34;, labour$name) # regex to find ending numbers and storing in a new var head(labour$year) 1 ## [1] \u0026#34;2021\u0026#34; \u0026#34;2022\u0026#34; \u0026#34;2023\u0026#34; \u0026#34;2006\u0026#34; \u0026#34;2008\u0026#34; \u0026#34;2011\u0026#34; Then we strip the name:\n1 2 labour$name_no_year \u0026lt;- trimws(gsub(\u0026#34;[[:punct:]]|[[:digit:]]\u0026#34;, \u0026#34;\u0026#34;, labour$name)) # regex to remove punctuation and remaing numbers head(labour$name_no_year) 1 2 ## [1] \u0026#34;Labour Force Survey\u0026#34; \u0026#34;Labour Force Survey\u0026#34; \u0026#34;Labour Force Survey\u0026#34; ## [4] \u0026#34;Labour Force Survey\u0026#34; \u0026#34;Labour Force Survey\u0026#34; \u0026#34;Labour Force Survey\u0026#34; And then look at the unique values:\n1 (unique(labour$name_no_year)) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ## [1] \u0026#34;Labour Force Survey\u0026#34; ## [2] \u0026#34;Survey of Labour and Income Dynamics\u0026#34; ## [3] \u0026#34;Survey of Persons Not in the Labour Force\u0026#34; ## [4] \u0026#34;Labour Force Income Profiles\u0026#34; ## [5] \u0026#34;Cultural Labour Force Survey\u0026#34; ## [6] \u0026#34;Small Area Business and Labour Database\u0026#34; ## [7] \u0026#34;Survey of Labour and Income Dynamics Reweighted\u0026#34; ## [8] \u0026#34;Labour Force Historical Review\u0026#34; ## [9] \u0026#34;Labour Force Survey Rebased Revised\u0026#34; ## [10] \u0026#34;Labour Force Survey revision\u0026#34; ## [11] \u0026#34;Interwar Labour Database\u0026#34; ## [12] \u0026#34;Labour force historical review\u0026#34; ## [13] \u0026#34;Labour Income Profile for selected BC communities\u0026#34; ## [14] \u0026#34;Labour Market Activity Survey\u0026#34; ## [15] \u0026#34;Experienced Labour Force Population by Occupation custom tabulation\u0026#34; ## [16] \u0026#34;Experienced Labour Force Population by Industry and Occupation custom tabulation\u0026#34; ## [17] \u0026#34;Labour Market Activity Survey Crosssectional Files\u0026#34; ## [18] \u0026#34;Labour Market Activity Survey Longitudinal Files\u0026#34; ## [19] \u0026#34;New Survey of London Life and Labour\u0026#34; ## [20] \u0026#34;Labour Force years of age and older in private households by occupation for Quebec Census of Canada Sample Custom tabulation\u0026#34; We only need the Labour Force Surveys themselves. These are occasionally revised, documentation on these revisions here, hence multiple records with slightly modified names (Labour Force Survey, Labour Force Survey (Rebased, Revised), etc)\n1 2 lfs \u0026lt;- labour[grepl(\u0026#34;^Labour Force Survey\u0026#34;, labour$name_no_year),] # regex to find names starting with LFS head(lfs) name description file_count producers pub_date handle global_id year name_no_year Labour Force Survey, 2021 LFS data are used to produce the well-known ... 91 Statistics Canada 2023-03-16T22:00:56Z https://hdl.handle.net/11272.1/AB2/HP9TEK hdl:11272.1/AB2/HP9TEK 2021 Labour Force Survey Labour Force Survey, 2022 LFS data are used to produce the well-known ... 57 Statistics Canada 2023-03-16T22:01:02Z https://hdl.handle.net/11272.1/AB2/SRAU7E hdl:11272.1/AB2/SRAU7E 2022 Labour Force Survey Labour Force Survey, 2023 LFS data are used to produce the well-known ... 31 Statistics Canada 2024-01-05T17:38:25Z https://hdl.handle.net/11272.1/AB2/IJU1QK hdl:11272.1/AB2/IJU1QK 2023 Labour Force Survey Labour Force Survey, 2006 The Labour Force Survey provides estimates o... 26 Statistics Canada 2023-03-16T21:47:04Z https://hdl.handle.net/11272.1/AB2/0B5LPL hdl:11272.1/AB2/0B5LPL 2006 Labour Force Survey Labour Force Survey, 2008 The Labour Force Survey provides estimates o... 26 Statistics Canada 2023-03-16T21:47:20Z https://hdl.handle.net/11272.1/AB2/LA3WXI hdl:11272.1/AB2/LA3WXI 2008 Labour Force Survey Labour Force Survey, 2011 The Labour Force Survey provides estimates o... 26 Statistics Canada 2023-03-16T21:47:47Z https://hdl.handle.net/11272.1/AB2/GK3SFF hdl:11272.1/AB2/GK3SFF 2011 Labour Force Survey Looking at Files We now know the data sets we\u0026rsquo;re interested in. A dataset is made up of multiple files, however, and we likely don\u0026rsquo;t want all the files for every dataset. We\u0026rsquo;ll start by getting the file list for one of our datasets. Once we can do this for one dataset, we can iterate over all the datasets, although we won\u0026rsquo;t implement that in this workshop.\nUp until now, we\u0026rsquo;ve been using the search api. We\u0026rsquo;ll now use the datasets api.\n1 datasets_api \u0026lt;- \u0026#34;datasets/\u0026#34; Like before, we\u0026rsquo;ll build our query. We\u0026rsquo;ll start with the 2011 LFS dataset. This is a multi-step process, as the first thing we need to do is get the id for the dataset, which is different from the global_id we retrieved from the search api, but we need the global_id, piped into the datasets api, to get the id.\n1 2 3 id \u0026lt;- lfs[lfs$name == \u0026#34;Labour Force Survey, 2011\u0026#34;, \u0026#34;global_id\u0026#34;, drop = TRUE] # we want a simple vector returned query \u0026lt;- paste0(\u0026#34;:persistentId/?persistentId=\u0026#34;, id) # limit to the id of interest, see dataverse API documentation (dataset_api_call \u0026lt;- paste0(base_url, datasets_api, query)) # build the call 1 ## [1] \u0026#34;https://abacus.library.ubc.ca/api/datasets/:persistentId/?persistentId=hdl:11272.1/AB2/GK3SFF\u0026#34; Now we make the call and process the body.\n1 2 3 4 dataset_id_resp \u0026lt;- dataset_api_call |\u0026gt; httr2::request() |\u0026gt; httr2::req_perform() dataset_id_body \u0026lt;- jsonlite::fromJSON(rawToChar(dataset_id_resp$body)) We need the id and the versionNumber to access a file list.\nBuilding the call:\n1 2 3 dataset_id \u0026lt;- dataset_id_body$data$id dataset_ver \u0026lt;- dataset_id_body$data$latestVersion$versionNumber (file_list_query \u0026lt;- paste0(base_url, datasets_api, dataset_id, \u0026#34;/versions/\u0026#34;, dataset_ver, \u0026#34;/files/\u0026#34;)) 1 ## [1] \u0026#34;https://abacus.library.ubc.ca/api/datasets/45508/versions/2/files/\u0026#34; Then execute, this time using the httr2 option, resp_body_json() to convert the body to an R object, as it\u0026rsquo;s a bit more streamlined.\n1 2 3 4 dataset_file_resp \u0026lt;- file_list_query |\u0026gt; httr2::request() |\u0026gt; httr2::req_perform() |\u0026gt; httr2::resp_body_json() Files are generally indicated as being either data, documentation, or command files. We\u0026rsquo;ll isolate just the data files in the list. All relevant data files have a directoryLabel equal to Data. We can use this to access only the data files.\n1 data_files \u0026lt;- Filter(function(x) x$directoryLabel == \u0026#34;Data\u0026#34;, dataset_file_resp$data) # filter by directoryLabel We\u0026rsquo;ll list out the files\n1 2 3 for(i in 1:length(data_files)){ print(data_files[[i]]$label) } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ## [1] \u0026#34;LFS_April_2011.tab\u0026#34; ## [1] \u0026#34;LFS_August_2011.tab\u0026#34; ## [1] \u0026#34;LFS_December_2011.tab\u0026#34; ## [1] \u0026#34;LFS_February_2011.tab\u0026#34; ## [1] \u0026#34;LFS_January_2011.tab\u0026#34; ## [1] \u0026#34;LFS_July_2011.tab\u0026#34; ## [1] \u0026#34;LFS_June_2011.tab\u0026#34; ## [1] \u0026#34;LFS_March_2011.tab\u0026#34; ## [1] \u0026#34;LFS_May_2011.tab\u0026#34; ## [1] \u0026#34;LFS_November_2011.tab\u0026#34; ## [1] \u0026#34;LFS_October_2011.tab\u0026#34; ## [1] \u0026#34;LFS_September_2011.tab\u0026#34; ## [1] \u0026#34;pub0111.prn\u0026#34; ## [1] \u0026#34;pub0211.prn\u0026#34; ## [1] \u0026#34;pub0311.prn\u0026#34; ## [1] \u0026#34;pub0411.prn\u0026#34; ## [1] \u0026#34;pub0511.prn\u0026#34; ## [1] \u0026#34;pub0611.prn\u0026#34; ## [1] \u0026#34;pub0711.prn\u0026#34; ## [1] \u0026#34;pub0811.prn\u0026#34; ## [1] \u0026#34;pub0911.prn\u0026#34; ## [1] \u0026#34;pub1011.prn\u0026#34; ## [1] \u0026#34;pub1111.prn\u0026#34; ## [1] \u0026#34;pub1211.prn\u0026#34; From which we\u0026rsquo;ll see that there are two different formats available to us. Let\u0026rsquo;s grab just the .tab files.\n1 relevant_data_files \u0026lt;- Filter(function(x) grepl(\u0026#34;tab$\u0026#34;, x$label), data_files) # filter to files ending in tab And then download them.\n1 dir.create(\u0026#34;LFS_2011\u0026#34;) # create a directory 1 ## Warning in dir.create(\u0026#34;LFS_2011\u0026#34;): \u0026#39;LFS_2011\u0026#39; already exists 1 2 3 4 5 6 7 8 9 dest_dir \u0026lt;- \u0026#34;LFS_2011/\u0026#34; # store it in a variable # loop through the files and download them for(i in 1:length(relevant_data_files)){ file_name \u0026lt;- relevant_data_files[[i]]$label file_id \u0026lt;- relevant_data_files[[i]]$dataFile$id url_root \u0026lt;- \u0026#34;https://abacus.library.ubc.ca/api/access/datafile/\u0026#34; download.file(url = paste0(url_root, file_id), destfile = paste0(dest_dir, file_name)) } And build an annual set\n1 2 3 4 5 lfs_2011_list \u0026lt;- lapply(list.files(dest_dir), function(x) read.delim(file = paste0(dest_dir, x))) # read in each tab file into a list lfs_2011_df \u0026lt;- do.call(rbind, lfs_2011_list) # bind the list items into a df summary(lfs_2011_df[, c(1:11)]) |\u0026gt; kbl() |\u0026gt; kable_styling(bootstrap_options = \u0026#34;striped\u0026#34;) REC_NUM SURVYEAR SURVMNTH LFSSTAT PROV CMA AGE_12 AGE_6 SEX MARSTAT EDUC Min. : 1 Min. :2011 Min. : 1.000 Min. :1.00 Min. :10.0 Min. :0.000 Min. : 1.000 Min. :1.0 Min. :1.000 Min. :1.000 Min. :0.000 1st Qu.: 26294 1st Qu.:2011 1st Qu.: 4.000 1st Qu.:1.00 1st Qu.:24.0 1st Qu.:0.000 1st Qu.: 4.000 1st Qu.:2.0 1st Qu.:1.000 1st Qu.:1.000 1st Qu.:2.000 Median : 52587 Median :2011 Median : 7.000 Median :1.00 Median :35.0 Median :0.000 Median : 7.000 Median :4.0 Median :2.000 Median :2.000 Median :3.000 Mean : 52589 Mean :2011 Mean : 6.509 Mean :2.19 Mean :35.1 Mean :1.473 Mean : 6.765 Mean :3.5 Mean :1.516 Mean :2.834 Mean :3.008 3rd Qu.: 78880 3rd Qu.:2011 3rd Qu.: 9.000 3rd Qu.:4.00 3rd Qu.:47.0 3rd Qu.:2.000 3rd Qu.:10.000 3rd Qu.:5.0 3rd Qu.:2.000 3rd Qu.:6.000 3rd Qu.:4.000 Max. :106352 Max. :2011 Max. :12.000 Max. :4.00 Max. :59.0 Max. :9.000 Max. :12.000 Max. :6.0 Max. :2.000 Max. :6.000 Max. :6.000 NA NA NA NA NA NA NA NA's :975211 NA NA NA ","date":"2025-02-14T14:00:00-08:00","permalink":"http://localhost:5082/csc-data-blog/p/abacus-api/","title":"Abacus API"},{"content":" Transforming response variables Data transformations are often taught in introductory quantitative and statistics courses as methods for dealing with non-Gaussian (i.e., not normal) data, whether it be bounded (e.g, strictly positive) or skewed (https://ubco-biology.github.io/BIOL202/transform.html; https://ubco-biology.github.io/BIOL202/LSR.html#transform-the-data; Section 13.3 in Whitlock \u0026amp; Schluter 2020). In this post, I use simulated data to demonstrate issues that might arise with transforming data. I use a simulated dataset rather than an empirical one so we can compare the model results to the truth.\nImagine that a dangerous intersection has been causing frequent accidents between motored vehicles. A local group of concerned citizens decides to count the number of monthly accidents and report it to City Council. City Council decides to intervene by placing better street signs and lighting. The group of citizens then decided to count the number of accidents again to test whether or not there was an improvement in the number of accidents.\nFor the sake of this post, let\u0026rsquo;s assume that the mean number of accidents in the 12 months before the intervention was \\(\\lambda_0=2\\) per month. In the 12 months after the intervention, the mean number of monthly accidents decreases to \\(\\lambda_1 = 0.4\\). We can then simulate data for before and after the intervention as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 library(\u0026#39;dplyr\u0026#39;) # for data wrangling library(\u0026#39;tidyr\u0026#39;) # for data wrangling library(\u0026#39;mgcv\u0026#39;) # for modeling library(\u0026#39;ggplot2\u0026#39;) # for fancy plots library(\u0026#39;gratia\u0026#39;) # for diagnostic plots in ggplot # change default ggplot theme theme_set(theme_bw() + theme(panel.grid = element_blank(), legend.position = \u0026#39;top\u0026#39;, text = element_text(face = \u0026#39;bold\u0026#39;))) set.seed(2024+10+7) # for reproducible results d \u0026lt;- expand_grid(month = 1:12, period = factor(c(\u0026#39;before\u0026#39;, \u0026#39;after\u0026#39;), levels = c(\u0026#39;before\u0026#39;, \u0026#39;after\u0026#39;))) %\u0026gt;% mutate(lambda = if_else(period == \u0026#39;before\u0026#39;, true = 2, false = 0.4), accidents = rpois(n = n(), lambda = lambda)) ggplot(d) + facet_wrap(~ period, ncol = 1) + geom_histogram(aes(accidents), binwidth = 1, fill = \u0026#39;grey\u0026#39;, color = \u0026#39;black\u0026#39;) + geom_vline(aes(xintercept = lambda), color = \u0026#39;darkorange\u0026#39;, lwd = 1) + labs(x = \u0026#39;Accidents per month\u0026#39;, y = \u0026#39;Count\u0026#39;) The distribution of the number of monthly accidents is clearly non-Gaussian both before and after the intervention. (Also note that the number of accidents cannot be negative.) Still, let\u0026rsquo;s fit a linear model to estimate the mean number of accidents in each period. In this post I use the mgcv::gam() function to fit all models for consistency and convenience, but note that you could also use the stats::lm() and stats::glm() functions, too. After fitting the model, we can look at the estimated change in mean accidents and appraise the model using diagnostic plots.\n1 2 m_1 \u0026lt;- gam(accidents ~ period, data = d) coef(m_1) 1 2 ## (Intercept) periodafter ## 1.666667 -1.250000 From the output of the coef() function, we can see that the estimated model is $$Y = 1.67 - 1.25\\,x_1,$$ where \\(x_1\\) is 0 for before the intervention and 1 for after. The periodafter coefficient indicates that there were -1.25 less accidents per month after the intervention. While this does not match the true effect of \\(2 - 0.4 = 1.6\\), we should expect some random variation, and the estimate is still reasonable. Longer observational periods would give us more accurate estimates.\n1 appraise(m_1, point_alpha = 0.3) Those who are used to looking at these plots may recognize two issues: (1) the residuals are not Gaussian (see the q-q plot and the histogram, in the left column), and (2) the variance in the observed values is lower when the predicted mean (linear predictor and fitted values) is lower. One may then decide to log-transform the number of accidents, after adding a small value to avoid taking the log of zero (since \\(\\log(0) = - \\infty\\)).\n1 2 3 4 # find proportion of months with no accidents d %\u0026gt;% group_by(period) %\u0026gt;% summarize(prop_zero = mean(accidents == 0)) 1 2 3 4 5 ## # A tibble: 2 × 2 ## period prop_zero ## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 before 0.167 ## 2 after 0.667 1 2 3 4 # add a variable of log(accidents + 1) d$log1p_accidents \u0026lt;- log1p(d$accidents) m_2 \u0026lt;- gam(log1p_accidents ~ period, data = d) coef(m_2) 1 2 ## (Intercept) periodafter ## 0.8620910 -0.5972532 1 appraise(m_2, point_alpha = 0.3) The log1p transformation resulted in a somewhat more symmetrical distribution of residuals, and the mode of the distribution is closer to zero, and improved the issue of non-constant variance (heteroskedasticity). The coefficients are also harder to interpret because they are on the log1p scale, since the model is $$\\log(Y+1)=0.86 - 0.60 \\,x_1.$$\nRather than forcing our response variable to be Gaussian when it isn\u0026rsquo;t (see this modeling workshop I created with the CSC), we can fit a model that fits our data well: a Generalized Linear Model (GLM). GLMs allow one to recognize the structure of the response variable (e.g., any bounds and the mean-variance relationship) by specifying an appropriate statistical distribution (e.g., Gaussian, binomial, etc.).\nThe Poisson distribution is a good option because it represents the number of events that occur within a given period of time and location. We can fit a Poisson GLM by specifying the family in the gam() function:\n1 2 m_3 \u0026lt;- gam(accidents ~ period, data = d, family = poisson(link = \u0026#39;log\u0026#39;)) coef(m_3) 1 2 ## (Intercept) periodafter ## 0.5108256 -1.3862944 1 appraise(m_3, point_alpha = 0.3) The output from coef() tells us that the GLM is $$\\lambda = e{0.51-1.39\\,x_1}.$$ This means that after the intervention, the number of accidents decreased to \\(e^-1.39 * 100%= 0.25%\\) what they were in the 12 months before.\nWith the GLM, we can see a improvement in both the q-q plot and histogram of residuals, and while the variance still increases with the mean number of accidents, the model now expects that to be the case. This is because the variance of a Poisson distributions is equal to its mean, both of which are usually indicated as \\(\\lambda\\) rather than \\(\\lambda\\).\nThe \\(\\log\\) link function (link = 'log') allows us to create a model that will only give us non-negative numbers of accidents by having the form $$\\log(\\lambda) = 0.51-1.39 \\, x_1,$$ which we can re-write as $$\\lambda = e^{ 0.51-1.39 \\, x_1}.$$ Notice that while this model may look similar to the log1p model, the \\(\\log\\) function is applied to \\(\\lambda\\), the mean of \\(Y\\), rather than to \\(Y\\) directly. This is important because it removes the systematic bias that results from applying non-linear transformations such as \\(\\log(Y)\\), \\(\\sqrt{Y}\\), and \\(\\arcsin\\left(\\sqrt{Y}\\right)\\) to a random variable. Now that we have three models, let\u0026rsquo;s compare between the predictions that the tree give us:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 # predictions pred \u0026lt;- function(model, model_type) { p \u0026lt;- predict(model, newdata = newdata, se.fit = TRUE) %\u0026gt;% as.data.frame() p \u0026lt;- p %\u0026gt;% transmute( model = model_type, # get predictions and approximate 95% Bayesian Credible Intervals est = case_when(model == \u0026#39;LM\u0026#39; ~ fit, grepl(\u0026#39;log1p\u0026#39;, model) ~ exp(fit) - 1, model == \u0026#39;Poisson GLM\u0026#39; ~ exp(fit)), lwr_95 = case_when(model == \u0026#39;LM\u0026#39; ~ fit - 1.96 * se.fit, grepl(\u0026#39;log1p\u0026#39;, model) ~ exp(fit - 1.96 * se.fit) - 1, model == \u0026#39;Poisson GLM\u0026#39; ~ exp(fit - 1.96 * se.fit)), upr_95 = case_when(model == \u0026#39;LM\u0026#39; ~ fit + 1.96 * se.fit, grepl(\u0026#39;log1p\u0026#39;, model) ~ exp(fit + 1.96 * se.fit) - 1, model == \u0026#39;Poisson GLM\u0026#39; ~ exp(fit + 1.96 * se.fit))) %\u0026gt;% bind_cols(newdata, .) %\u0026gt;% mutate(x = paste(period, model)) p } newdata \u0026lt;- tibble(period = unique(d$period)) preds \u0026lt;- bind_rows( pred(model = m_1, model_type= \u0026#39;LM\u0026#39;), pred(model = m_2, model_type= \u0026#39;LM of log1p\u0026#39;), pred(model = m_3, model_type= \u0026#39;Poisson GLM\u0026#39;)) ggplot() + facet_wrap(~ period, nrow = 1) + geom_hline(yintercept = 0, color = \u0026#39;grey\u0026#39;) + geom_hline(aes(yintercept = lambda), color = \u0026#39;black\u0026#39;, d, lty = \u0026#39;dashed\u0026#39;) + geom_jitter(aes(\u0026#39;LM of log1p\u0026#39;, accidents), d, alpha = 0.3, height = 0) + geom_errorbar(aes(model, ymin = lwr_95, ymax = upr_95, color = model), preds, alpha = 0.3, width = 0) + geom_point(aes(model, est, color = model), preds) + labs(x = NULL, y = \u0026#39;Number of accidents\u0026#39;) + khroma::scale_color_highcontrast(name = \u0026#39;Model\u0026#39;) From the plot above, we can see that the LM and GLM have fairly similar coefficient estimates and that the log1p LM resulted in lower estimates for both. This is due to a systematic bias that occurs when one applies a nonlinear transformation to a random variable, like \\(\\log(Y)\\). I explain why in the sections below. Additionally, while the LM\u0026rsquo;s coefficients are reasonable, the 95% Bayesian Credible Intervals (BCIs) for the post-intervention estimate are not appropriate because they include negative values. In contrast, the GLM\u0026rsquo;s BCIs are asymmetric: they are shorter below the mean and longer above the mean. This is recognizes that a multiplicative change for large values is larger than one for small values: a 50% decrease will take 2 to 1 and take 4 to 2. Consequently, asymmetric BCIs provide a better representation of the model\u0026rsquo;s uncertainty.\nLinear and nonlinear transformations I want to end this section by explaining the difference between linear and nonlinear transformations and how the two impact modeling. Linear transformations include all transformations that can be written as a series of additions, subtractions, multiplications, or divisions. Formally, they have the form \\(Y_t = (aY + b)\\), where \\(Y\\) is our original response variable of interest, \\(a\\) and \\(b\\) are any number (but \\(a \\ne 0\\)), and \\(Y_t\\) is the transformed response. These transformations are called \u0026ldquo;linear\u0026rdquo; because the operations of addition, subtraction, multiplication and division only shift distributions left and right (addition and subtraction) or expand and shrink distributions (multiplication and division), so \\(Y_t\\) and \\(Y\\) follow a linear relationship for any finite and nonzero value of \\(a\\) and any finite value of \\(b\\):\n1 2 Y \u0026lt;- seq(0.01, 2, by = 1e-3) ggplot() + geom_line(aes(Y, Y/10 + 3)) Conversely, nonlinear transformations are all transformations that cannot be written using only addition, subtraction, multiplications, or division. If we were to plot any of these functions, we would see a nonlinear relationship between the original and transformed values:\n1 2 3 4 5 6 7 8 9 10 11 12 expand_grid(Y = Y, trans = c(\u0026#39;Y / 10 + 3\u0026#39;, \u0026#39;log(Y)\u0026#39;, \u0026#39;sqrt(Y)\u0026#39;, \u0026#39;arcsin(sqrt(Y))\u0026#39;)) %\u0026gt;% mutate( Y_star = case_when(trans == \u0026#39;Y / 10 + 3\u0026#39; ~ Y/10 + 3, trans == \u0026#39;log(Y)\u0026#39; ~ log(Y), trans == \u0026#39;sqrt(Y)\u0026#39; ~ sqrt(Y), trans == \u0026#39;arcsin(sqrt(Y))\u0026#39; ~ asin(sqrt(Y)))) %\u0026gt;% ggplot(aes(Y, Y_star)) + facet_wrap(~ trans, scales = \u0026#39;free_y\u0026#39;) + geom_line() + labs(x = \u0026#39;Y\u0026#39;, y = expression(bold(Y[t]))) 1 2 3 4 ## Warning: There was 1 warning in `mutate()`. ## ℹ In argument: `Y_star = case_when(...)`. ## Caused by warning in `asin()`: ## ! NaNs produced Jensen\u0026rsquo;s inequality Jensen\u0026rsquo;s inequality demostrates that if we apply a nonlinear transformation to a random variable, we can\u0026rsquo;t simply back-transform the mean. This is because, if \\(g()\\) is a nonlinear transformation and \\(g^{-1}()\\) is its inverse (e.g, \\(g() = \\log()\\) and \\(g^{-1}()=\\exp()\\)): $$g\\big[\\mathbb E(Y)\\big] \\ne \\mathbb E\\big[g(Y)\\big],$$ where \\(\\mathbb E()\\) indicates an expected value such that \\(\\mathbb E(Y)=\\mu\\). This implies that $$\\mathbb E(Y) = \\mu \\ne g^{-1}\\bigg\\{\\mathbb E\\big[g(Y)\\big]\\bigg\\}.$$\nHere is a graphical example to help you visualize why. The green vertical line is the estimated mean (i.e., what we\u0026rsquo;re usually interested in), the blue line is the estimated mean of the square-root transformed \\(Y\\), and the blue line is the back-transformed estimated mean of the square-root transformed \\(Y\\). The black line is the estimated bias.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 set.seed(2024-10-7) Y \u0026lt;- rpois(n = 250, lambda = 1) ggExtra::ggMarginal( ggplot() + geom_line(aes(seq(0, ceiling(max(Y)), by = 0.01), y = sqrt(after_stat(x))), color = \u0026#39;grey\u0026#39;) + geom_jitter(aes(Y, sqrt(Y)), height = 0.02, alpha = 0.3) + geom_vline(aes(color = \u0026#39;estimated~E(Y)\u0026#39;, xintercept = mean(Y)), lwd = 1) + geom_hline(aes(color = \u0026#39;estimated~E(sqrt(Y))\u0026#39;, yintercept = mean(sqrt(Y))), lwd = 1) + geom_vline(aes(color = \u0026#39;estimated~(E(sqrt(Y)))^2\u0026#39;, xintercept = mean(sqrt(Y))^2), lwd = 1) + geom_segment(aes(x = mean(sqrt(Y))^2, xend = mean(Y), y = 0.5, yend = 0.5), arrow = grid::arrow(ends = \u0026#39;both\u0026#39;, length = unit(0.1, \u0026#39;in\u0026#39;))) + labs(x = \u0026#39;Y\u0026#39;, y = expression(bold(sqrt(Y)))) + khroma::scale_color_bright(name = \u0026#39;Variable\u0026#39;, labels = scales::parse_format()) + theme(legend.position = \u0026#39;inside\u0026#39;, legend.position.inside = c(0.8, 0.2), legend.frame = element_rect(fill = \u0026#39;black\u0026#39;)), fill = \u0026#39;grey\u0026#39;, type = \u0026#39;histogram\u0026#39;, xparams = list(bins = 5), yparams = list(bins = 5)) However, note that the transformations only cause bias if applied to random variables. You can apply nonlinear transformations to predictor variables that you assume are measured exactly right. This means that fitting a model of the form \\(Y = \\beta_0 + \\beta_1 \\log(x_1)\\) will not cause issues if you assume \\(x_1\\) is exactly correct with no error (as we often assume). Applying nonlinear transformations on predictors can be useful when a predictor has a distribution with a very long tail, or if one wants to create a variable with diminishing effects (e.g., moving closer to something matters more when you\u0026rsquo;re close and less when you\u0026rsquo;re already far from it). I will make a blog post on this in the future.\nReferences Whitlock M. \u0026amp; Schluter D. (2020). Analysis of biological data, Third edition. Macmillan Learning, New York, NY.\n","date":"2024-10-07T14:00:00-08:00","permalink":"http://localhost:5082/csc-data-blog/p/transforming-data/","title":"Transforming data"},{"content":"GitHub is a platform that allows you to store, share, and archive files. It is commonly used for in research to make files accessible, including code, data, models, and figures. Sharing code and data through platforms like GitHub helps make research more transparent, reproducible, and credible Braga et al. 2023. In this blog post, I will show you how to create your first GitHub repository (\u0026ldquo;repo\u0026rdquo; for short), how to set up a convenient folder structure, and how to push (i.e., upload) and pull (i.e., download) content to and from the repo. The Center for Scholarly Communication (CSC) also runs a two-part workshop on how to use GitHub. The material is available on the CSC website: Part 1 and Part 2.\nBefore we start, you will need to download and install GitHub Desktop from https://desktop.github.com/download/. While it installs, make an account on github.com if you don\u0026rsquo;t have one already.\nTo make your first repo, go to File/New repository or press Ctrl + N.\nNow choose your repo\u0026rsquo;s name. Something short and simple, like my-first-repo works for now. If you want, you can also add a description of the repo\u0026rsquo;s contents and change the location of the local copy that GitHub Desktop will make. On my personal machine, I tend to save repos in Documents/GitHub, but on my lab machine I save them in my UBC home drive: H:/GitHub. If you are planning on working primarily with only one language (e.g., R or Python), I would suggest choosing the appropriate Git ignore. This will hide files that you generally don\u0026rsquo;t want to push to GitHub (e.g., .Rhistory and .RData files). You can change the .gitignore files later with any text editor. The last step here is to choose a license. If you want a simple open-access licence for code, the MIT one is a good option.\nYou\u0026rsquo;ve now created your first GitHub repo, but it is currently only on your local machine. To push it to GitHub, you\u0026rsquo;ll need to publish the repo (top ribbon, to the right). Before confirming, you can:\nupdate the name (this won\u0026rsquo;t change the name of the folder on your machine), edit the description, decide whether you want the repo to be public or not (it\u0026rsquo;s private by default), and choose if you want to add it into an existing organization (e.g., your lab\u0026rsquo;s organization). Be careful though \u0026ndash; it can be hard to edit the repo after you\u0026rsquo;ve added it to an organization. Now that the repo is on GitHub, you can start pushing and pulling content to and from it. As a general rule, you should always check for any updates by pulling (click Fetch-origin followed by Pull origin) before trying to push anything, otherwise GitHub will have to manage the pull/push conflict by merging the old files (to be pulled) with the new ones (to be pushed). This can be quite confusing, so it\u0026rsquo;s always good to make to pull first (even if you\u0026rsquo;re the only one working on the repo on two machines). Once you have some files you want to push to the repo, select the ones you want to push by clicking on the boxes in the upper left, add a summary for the commit (i.e., the series of changes to push) at the bottom of the column. Commit summaries should be brief and simple but clear, like \u0026ldquo;cleaned data and started model fitting\u0026rdquo;. You can use markdown in both the summary and the description (see the field below the summary field) for messages like \u0026ldquo;removed unnecessary library() calls\u0026rdquo; using \\(\\text{text}\\). In the changes column, green \u0026ldquo;+\u0026rdquo; symbols indicate new files, yellow dots indicate changed files, and red \u0026ldquo;-\u0026rdquo; symbols indicate file deletions. The panel on the right will show you exactly what portions of a file you changed, if the file is a simple text file (e.g., .csv., .R, .Rmd, .md, and .txt, but not .pdf, .doc, .ppt, or .rds). Consequently, GitHub\u0026rsquo;s version control and edit history work best if you use simple text files as much as possible. Once you\u0026rsquo;ve committed all your changes, don\u0026rsquo;t forget to push the changes to GitHub using the \u0026ldquo;Push origin\u0026rdquo; button! You can check the status of your changes by going to GitHub and navigating to your repo.\nReferences Braga P.H.P., Hébert K., Hudgins E.J., Scott E.R., Edwards B.P.M., Sánchez Reyes L.L., et al. (2023). Not just for programmers: How GitHub can accelerate collaborative and reproducible research in ecology and evolution. Methods in Ecology and Evolution 14, 1364–1380. https://doi.org/10.1111/2041-210X.14108\n","date":"2024-09-16T12:00:00-08:00","permalink":"http://localhost:5082/csc-data-blog/p/getting-started-with-github/","title":"Getting started with GitHub"},{"content":"Creating Sample Datasets This guide provides instructions on how to create sample datasets in R and Python. You can use these methods to generate a mini version of your original dataset for data consultations, enabling efficient and effective analysis on a manageable subset of your data. We assume you know how to read in your data, however, if you need step by step instructions on this, these are available further down the page for both R and Python.\nR Prerequisites\nYou will need dplyr installed. You can double check that you have it installed:\n1 find.package(\u0026#34;dplyr\u0026#34;) # returns an error if the package is not installed, else returns the path to the package 1 ## [1] \u0026#34;/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/dplyr\u0026#34; And install it if necessary:\n1 install.packages(\u0026#34;dplyr\u0026#34;) # REQUIRED We want a minimum of 10 samples per variable or a maximum of 40% of your data if there is concern that 10 samples per variable will be insufficient for demonstration purposes. We want to store the output as an R object.\n10 samples per variable\nReplace your_data_frame in the second line with the name you assigned to your data on import. Replace \u0026quot;path/to/your/file.RData\u0026quot; in the last line with the path and file name to save your sampled data to. 1 2 3 4 5 library(dplyr) df_to_sample \u0026lt;- your_data_frame n_samples \u0026lt;- ncol(df_to_sample) * 10 # calculate the number of variables in your data frame and multiply by 10 sampled_data \u0026lt;- df_to_sample %\u0026gt;% slice_sample(n_samples) # take the sample save(sampled_data, file = \u0026#34;path/to/your/file.RData\u0026#34;) # choose a location to save your RData file with the .RData extension Bring the resulting .RData file with you to your consultation.\n40% of your observations\nReplace your_data_frame in the second line with the name you assigned to your data on import. Replace \u0026quot;path/to/your/file.RData\u0026quot; in the last line with the path and file name to save your sampled data to. 1 2 3 4 5 library(dplyr) df_to_sample \u0026lt;- your_data_frame n_samples \u0026lt;- round(nrow(df_to_sample) * 0.4) # calculate the number of observations in your data frame and multiply by 0.4 sampled_data \u0026lt;- df_to_sample %\u0026gt;% slice_sample(n_samples) # take the sample save(sampled_data, file = \u0026#34;path/to/your/file.RData\u0026#34;) # choose a location to save your RData file with the .RData extension Bring the resulting .RData file with you to your consultation.\nPython We want a minimum of 10 samples per variable or a maximum of 40% of your data if there is concern that 10 samples per variable will be insufficient for demonstration purposes. We want to store the output as a csv file.\n10 samples per variable\nReplace your_data_frame in the second line with the name you assigned to your data on import. Replace \u0026quot;path/to/your/file.RData\u0026quot; in the last line with the path and file name to save your sampled data to. 1 2 3 4 5 import pandas as pd df_to_sample = your_data_frame n_samples = len(df_to_sample.columns) * 10 # calculate the number of variables in your data frame and multiply by 10 sampled_data = df_to_sample.sample(n = n_samples) # take the sample sampled_data.to_csv(\u0026#34;path/to/your/file.csv\u0026#34;) # choose a location to save your csv file with a .csv extension Bring the resulting .csv filw with you to your consultation.\n40% of your observations\nReplace your_data_frame in the second line with the name you assigned to your data on import. Replace \u0026quot;path/to/your/file.RData\u0026quot; in the last line with the path and file name to save your sampled data to. 1 2 3 4 5 import pandas as pd df_to_sample = your_data_frame n_samples = round(df_to_sample.shape[0] * 0.4) # calculate the number of observations in your data frame and multiply by 0.4 sampled_data = df_to_sample.sample(n = n_samples) # take the sample sampled_data.to_csv(\u0026#34;path/to/your/file.csv\u0026#34;) # choose a location to save your csv file with a .csv extension Importing data Importing Data into R Prerequisites\nMake sure you have the readr package for CSV, readxl package for Excel, or jsonlite package for JSON installed. If not, you can install them using:\n1 2 3 install.packages(\u0026#34;readr\u0026#34;) # if reading in csv or other delimited rectangular data install.packages(\u0026#34;readxl\u0026#34;) # if reading in Excel files install.packages(\u0026#34;jsonlite\u0026#34;) # if reading in JSON files Import CSV file: 1 2 library(readr) df_to_sample \u0026lt;- read_csv(\u0026#39;path/to/your/file.csv\u0026#39;) Import Excel file: 1 2 library(readxl) df_to_sample \u0026lt;- read_excel(\u0026#39;path/to/your/file.xlsx\u0026#39;) Import JSON file 1 2 library(jsonlite) df_to_sample \u0026lt;- fromJSON(\u0026#39;path/to/your/file.json\u0026#39;) Importing Data in Python Import CSV file: 1 2 import pandas as pd df_to_sample = pd.read_csv(\u0026#39;path/to/your/file.csv\u0026#39;) Import Excel file: 1 2 import pandas as pd df_to_sample = pd.read_excel(\u0026#39;path/to/your/file.xlsx\u0026#39;) Import JSON file: 1 2 import pandas as pd df_to_sample = pd.read_json(\u0026#39;path/to/your/file.json\u0026#39;) ","date":"2024-04-09T09:00:00-08:00","permalink":"http://localhost:5082/csc-data-blog/p/sample-data-generation/","title":"Sample Data Generation"},{"content":"In previous categories we have looked at cleaning, summarizing, and subsetting data, with some minor calculations, but we haven’t yet looked at analyzing our data.\nR is a very powerful tool for data analysis. We can fit linear models and view graphs. First, we will look at some basic data analysis processes in R.\nLet’s re-load in our Gapminder data:\n1 2 3 link \u0026lt;- \u0026#39;https://raw.githubusercontent.com/jstaf/gapminder/master/gapminder/gapminder.csv\u0026#39; df \u0026lt;- read.csv(url(link)) head(df) 1 2 3 4 5 6 7 ## country continent year lifeExp pop gdpPercap ## 1 Afghanistan Asia 1952 28.801 8425333 779.4453 ## 2 Afghanistan Asia 1957 30.332 9240934 820.8530 ## 3 Afghanistan Asia 1962 31.997 10267083 853.1007 ## 4 Afghanistan Asia 1967 34.020 11537966 836.1971 ## 5 Afghanistan Asia 1972 36.088 13079460 739.9811 ## 6 Afghanistan Asia 1977 38.438 14880372 786.1134 Linear Modelling Let’s say we want to fit a linear model to see if there is a relationship between population and time. First, let’s do this with the entire dataset with all countries.\nWe know that there is Simple Linear Regression (SLR) and Multiple Linear Regression (MLR). See the pseudocode below for a layout on how to form both in R.\n1 2 3 4 5 6 # Simple Linear Regression # model_name \u0026lt;- lm(Y ~ X, data = dataframe) # Multiple Linear Regression # model_name \u0026lt;- lm(Y ~ X1 + X2 + ..., data = dataframe) # model_name \u0026lt;- lm(Y ~ ., data = dataframe) to use all variables except Y Now that we have the layout of the code down, let\u0026rsquo;s use Y as population and X as year for an SLR model.\n1 2 model1 \u0026lt;- lm(pop ~ year, data = df) summary(model1) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## ## Call: ## lm(formula = pop ~ year, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -43318856 -27548179 -18558743 -9628265 1275164661 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -972185807 294031308 -3.306 0.000965 *** ## year 506081 148532 3.407 0.000672 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 105800000 on 1702 degrees of freedom ## Multiple R-squared: 0.006775,\tAdjusted R-squared: 0.006191 ## F-statistic: 11.61 on 1 and 1702 DF, p-value: 0.0006716 Now, let\u0026rsquo;s perform MLR with two variables, year and lifeExp.\n1 2 model2 \u0026lt;- lm(pop ~ year + lifeExp, data = df) summary(model2) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ## ## Call: ## lm(formula = pop ~ year + lifeExp, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -47148304 -27382074 -18454446 -8444157 1273829226 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -799315581 321078589 -2.489 0.0129 * ## year 409882 164973 2.485 0.0131 * ## lifeExp 295176 220507 1.339 0.1809 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 105800000 on 1701 degrees of freedom ## Multiple R-squared: 0.00782,\tAdjusted R-squared: 0.006653 ## F-statistic: 6.703 on 2 and 1701 DF, p-value: 0.00126 Finally, let\u0026rsquo;s perform MLR with all variables in the dataset.\n1 2 model3 \u0026lt;- lm(pop ~ ., data = df) summary(model3) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 ## ## Call: ## lm(formula = pop ~ ., data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -375691051 -5548380 -604356 4854327 391492520 ## ## Coefficients: (4 not defined because of singularities) ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -2.211e+08 1.677e+08 -1.319 0.187435 ## countryAlbania -5.423e+07 1.431e+07 -3.789 0.000157 *** ## countryAlgeria -2.412e+07 1.347e+07 -1.790 0.073575 . ## countryAngola -8.521e+06 1.259e+07 -0.677 0.498532 ## countryArgentina -2.796e+07 1.449e+07 -1.930 0.053788 . ## countryAustralia -4.731e+07 1.550e+07 -3.053 0.002304 ** ## countryAustria -5.220e+07 1.533e+07 -3.405 0.000678 *** ## countryBahrain -4.982e+07 1.443e+07 -3.453 0.000568 *** ## countryBangladesh 5.839e+07 1.287e+07 4.538 6.10e-06 *** ## countryBelgium -5.088e+07 1.537e+07 -3.310 0.000954 *** ## countryBenin -2.687e+07 1.282e+07 -2.096 0.036235 * ## countryBolivia -2.992e+07 1.302e+07 -2.299 0.021637 * ## countryBosnia and Herzegovina -5.198e+07 1.424e+07 -3.650 0.000271 *** ## countryBotswana -3.697e+07 1.316e+07 -2.808 0.005043 ** ## countryBrazil 7.429e+07 1.376e+07 5.400 7.68e-08 *** ## countryBulgaria -4.978e+07 1.450e+07 -3.432 0.000614 *** ## countryBurkina Faso -1.793e+07 1.268e+07 -1.414 0.157468 ## countryBurundi -2.106e+07 1.268e+07 -1.661 0.096887 . ## countryCambodia -2.130e+07 1.278e+07 -1.666 0.095911 . ## countryCameroon -2.008e+07 1.280e+07 -1.569 0.116755 ## countryCanada -3.735e+07 1.563e+07 -2.389 0.017010 * ## countryCentral African Republic -2.179e+07 1.266e+07 -1.722 0.085353 . ## countryChad -2.287e+07 1.274e+07 -1.795 0.072860 . ## countryChile -4.360e+07 1.427e+07 -3.056 0.002279 ** ## countryChina 9.099e+08 1.366e+07 66.602 \u0026lt; 2e-16 *** ## countryColombia -2.330e+07 1.388e+07 -1.679 0.093446 . ## countryComoros -3.532e+07 1.300e+07 -2.718 0.006646 ** ## countryCongo, Dem. Rep. 7.368e+06 1.267e+07 0.581 0.561055 ## countryCongo, Rep. -3.354e+07 1.302e+07 -2.576 0.010080 * ## countryCosta Rica -5.633e+07 1.453e+07 -3.876 0.000111 *** ## countryCote d\u0026#39;Ivoire -2.113e+07 1.281e+07 -1.650 0.099208 . ## countryCroatia -5.353e+07 1.460e+07 -3.666 0.000255 *** ## countryCuba -5.049e+07 1.464e+07 -3.448 0.000580 *** ## countryCzech Republic -4.891e+07 1.490e+07 -3.283 0.001050 ** ## countryDenmark -5.625e+07 1.554e+07 -3.620 0.000304 *** ## countryDjibouti -2.712e+07 1.274e+07 -2.129 0.033376 * ## countryDominican Republic -4.194e+07 1.366e+07 -3.071 0.002169 ** ## countryEcuador -4.060e+07 1.381e+07 -2.941 0.003322 ** ## countryEgypt 6.005e+06 1.325e+07 0.453 0.650446 ## countryEl Salvador -4.046e+07 1.352e+07 -2.993 0.002807 ** ## countryEquatorial Guinea -2.252e+07 1.264e+07 -1.781 0.075048 . ## countryEritrea -2.446e+07 1.271e+07 -1.924 0.054538 . ## countryEthiopia 1.638e+07 1.267e+07 1.293 0.196194 ## countryFinland -5.543e+07 1.520e+07 -3.648 0.000273 *** ## countryFrance -8.804e+06 1.541e+07 -0.571 0.567874 ## countryGabon -3.138e+07 1.310e+07 -2.396 0.016694 * ## countryGambia -2.432e+07 1.267e+07 -1.920 0.055049 . ## countryGermany 1.733e+07 1.538e+07 1.127 0.259883 ## countryGhana -2.306e+07 1.299e+07 -1.775 0.076090 . ## countryGreece -5.244e+07 1.515e+07 -3.460 0.000555 *** ## countryGuatemala -3.413e+07 1.329e+07 -2.567 0.010339 * ## countryGuinea -1.818e+07 1.264e+07 -1.438 0.150520 ## countryGuinea-Bissau -1.729e+07 1.258e+07 -1.374 0.169663 ## countryHaiti -2.725e+07 1.288e+07 -2.115 0.034566 * ## countryHonduras -3.889e+07 1.337e+07 -2.909 0.003672 ** ## countryHong Kong, China -5.631e+07 1.521e+07 -3.703 0.000220 *** ## countryHungary -4.642e+07 1.457e+07 -3.186 0.001473 ** ## countryIceland -6.410e+07 1.575e+07 -4.071 4.91e-05 *** ## countryIndia 6.643e+08 1.304e+07 50.949 \u0026lt; 2e-16 *** ## countryIndonesia 1.101e+08 1.311e+07 8.396 \u0026lt; 2e-16 *** ## countryIran -6.002e+05 1.349e+07 -0.045 0.964511 ## countryIraq -2.588e+07 1.334e+07 -1.939 0.052645 . ## countryIreland -5.722e+07 1.513e+07 -3.781 0.000162 *** ## countryIsrael -5.786e+07 1.515e+07 -3.819 0.000139 *** ## countryItaly -7.265e+06 1.527e+07 -0.476 0.634287 ## countryJamaica -5.453e+07 1.439e+07 -3.789 0.000157 *** ## countryJapan 4.916e+07 1.542e+07 3.187 0.001465 ** ## countryJordan -4.258e+07 1.351e+07 -3.151 0.001656 ** ## countryKenya -1.790e+07 1.301e+07 -1.376 0.169170 ## countryKorea, Dem. Rep. -3.402e+07 1.383e+07 -2.459 0.014037 * ## countryKorea, Rep. -1.476e+07 1.406e+07 -1.050 0.293722 ## countryKuwait -4.441e+07 1.832e+07 -2.424 0.015450 * ## countryLebanon -4.982e+07 1.412e+07 -3.528 0.000431 *** ## countryLesotho -3.122e+07 1.287e+07 -2.425 0.015431 * ## countryLiberia -2.074e+07 1.263e+07 -1.643 0.100642 ## countryLibya -3.974e+07 1.365e+07 -2.911 0.003652 ** ## countryMadagascar -1.960e+07 1.278e+07 -1.534 0.125294 ## countryMalawi -1.672e+07 1.264e+07 -1.322 0.186342 ## countryMalaysia -3.626e+07 1.393e+07 -2.602 0.009349 ** ## countryMali -1.668e+07 1.264e+07 -1.319 0.187231 ## countryMauritania -3.379e+07 1.299e+07 -2.600 0.009400 ** ## countryMauritius -5.093e+07 1.399e+07 -3.641 0.000280 *** ## countryMexico 1.651e+07 1.409e+07 1.172 0.241337 ## countryMongolia -3.857e+07 1.321e+07 -2.919 0.003558 ** ## countryMontenegro -5.799e+07 1.458e+07 -3.977 7.29e-05 *** ## countryMorocco -2.191e+07 1.334e+07 -1.642 0.100692 ## countryMozambique -7.711e+06 1.259e+07 -0.612 0.540438 ## countryMyanmar -3.676e+06 1.305e+07 -0.282 0.778138 ## countryNamibia -3.556e+07 1.308e+07 -2.719 0.006626 ** ## countryNepal -1.469e+07 1.283e+07 -1.145 0.252448 ## countryNetherlands -4.915e+07 1.569e+07 -3.132 0.001769 ** ## countryNew Zealand -5.847e+07 1.530e+07 -3.821 0.000138 *** ## countryNicaragua -4.023e+07 1.340e+07 -3.001 0.002732 ** ## countryNiger -1.845e+07 1.267e+07 -1.456 0.145611 ## countryNigeria 4.984e+07 1.265e+07 3.940 8.51e-05 *** ## countryNorway -5.822e+07 1.597e+07 -3.646 0.000275 *** ## countryOman -4.030e+07 1.359e+07 -2.965 0.003069 ** ## countryPakistan 5.468e+07 1.315e+07 4.159 3.37e-05 *** ## countryPanama -5.350e+07 1.429e+07 -3.745 0.000187 *** ## countryParaguay -5.108e+07 1.415e+07 -3.611 0.000315 *** ## countryPeru -2.598e+07 1.347e+07 -1.928 0.054039 . ## countryPhilippines 5.621e+06 1.360e+07 0.413 0.679421 ## countryPoland -2.383e+07 1.459e+07 -1.633 0.102628 ## countryPortugal -4.834e+07 1.470e+07 -3.289 0.001028 ** ## countryPuerto Rico -5.801e+07 1.494e+07 -3.882 0.000108 *** ## countryReunion -5.358e+07 1.415e+07 -3.785 0.000159 *** ## countryRomania -3.503e+07 1.437e+07 -2.438 0.014885 * ## countryRwanda -1.591e+07 1.261e+07 -1.262 0.207196 ## countrySao Tome and Principe -4.295e+07 1.335e+07 -3.217 0.001324 ** ## countrySaudi Arabia -2.802e+07 1.391e+07 -2.015 0.044058 * ## countrySenegal -2.684e+07 1.291e+07 -2.080 0.037702 * ## countrySerbia -4.703e+07 1.444e+07 -3.257 0.001151 ** ## countrySierra Leone -1.122e+07 1.258e+07 -0.892 0.372703 ## countrySingapore -5.517e+07 1.499e+07 -3.680 0.000241 *** ## countrySlovak Republic -5.370e+07 1.470e+07 -3.653 0.000268 *** ## countrySlovenia -5.719e+07 1.491e+07 -3.835 0.000130 *** ## countrySomalia -1.526e+07 1.260e+07 -1.211 0.226045 ## countrySouth Africa -6.780e+06 1.316e+07 -0.515 0.606543 ## countrySpain -2.663e+07 1.521e+07 -1.750 0.080268 . ## countrySri Lanka -3.997e+07 1.410e+07 -2.834 0.004649 ** ## countrySudan -8.692e+06 1.281e+07 -0.679 0.497470 ## countrySwaziland -3.013e+07 1.284e+07 -2.346 0.019114 * ## countrySweden -5.577e+07 1.568e+07 -3.557 0.000386 *** ## countrySwitzerland -5.543e+07 1.595e+07 -3.475 0.000525 *** ## countrySyria -3.750e+07 1.364e+07 -2.749 0.006043 ** ## countryTaiwan -4.115e+07 1.466e+07 -2.808 0.005049 ** ## countryTanzania -9.287e+06 1.278e+07 -0.726 0.467660 ## countryThailand -3.539e+06 1.371e+07 -0.258 0.796393 ## countryTogo -3.164e+07 1.295e+07 -2.443 0.014670 * ## countryTrinidad and Tobago -5.277e+07 1.423e+07 -3.709 0.000216 *** ## countryTunisia -3.975e+07 1.359e+07 -2.924 0.003502 ** ## countryTurkey 1.033e+06 1.352e+07 0.076 0.939115 ## countryUganda -1.504e+07 1.277e+07 -1.178 0.239162 ## countryUnited Kingdom -4.993e+06 1.538e+07 -0.325 0.745519 ## countryUnited States 1.690e+08 1.566e+07 10.794 \u0026lt; 2e-16 *** ## countryUruguay -5.631e+07 1.463e+07 -3.848 0.000124 *** ## countryVenezuela -3.789e+07 1.426e+07 -2.658 0.007945 ** ## countryVietnam 1.200e+07 1.332e+07 0.901 0.367650 ## countryWest Bank and Gaza -4.401e+07 1.356e+07 -3.245 0.001201 ** ## countryYemen, Rep. -1.729e+07 1.274e+07 -1.357 0.175082 ## countryZambia -2.077e+07 1.272e+07 -1.633 0.102627 ## countryZimbabwe -2.855e+07 1.301e+07 -2.194 0.028351 * ## continentAmericas NA NA NA NA ## continentAsia NA NA NA NA ## continentEurope NA NA NA NA ## continentOceania NA NA NA NA ## year 9.442e+04 8.804e+04 1.072 0.283681 ## lifeExp 1.339e+06 2.189e+05 6.118 1.19e-09 *** ## gdpPercap -1.908e+02 1.655e+02 -1.153 0.248975 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 30810000 on 1559 degrees of freedom ## Multiple R-squared: 0.9229,\tAdjusted R-squared: 0.9158 ## F-statistic: 129.6 on 144 and 1559 DF, p-value: \u0026lt; 2.2e-16 Note that the outputs give us a lot of information. We see the default hypotheses tested, with p-values for each coefficient and for the model as a whole. We see the RSE and the degrees of freedom, F-test, R-squared values, and much more! There are a lot of insights that we can extract from this simple output.\n","date":"2023-08-01T09:00:00-08:00","permalink":"http://localhost:5082/csc-data-blog/p/basic-model-fitting-in-r/","title":"Basic Model Fitting in R"},{"content":"Occasionally we need to derive variables form existing information. A good example of this is conversion between scales. If we wanted to change a measurement from metres to kilometres or weight in pounds to kilograms, we could do this simply by performing a basic operation on an entire column. In this situation, let’s start by multiplying two columns together to create a new column. Recall the GDP dataset from previous articles. Let’s say we want to know the total GDP for the country. This could be calculated by multiplying the population by the GDP_per_Cap.\nIf you are just starting here, let’s re-load in the Gapminder data that we have been using in previous posts:\n1 2 link \u0026lt;- \u0026#39;https://raw.githubusercontent.com/jstaf/gapminder/master/gapminder/gapminder.csv\u0026#39; df \u0026lt;- read.csv(url(link)) 1 2 df$Total_GDP \u0026lt;- df$pop * df$gdpPercap head(df) 1 2 3 4 5 6 7 ## country continent year lifeExp pop gdpPercap Total_GDP ## 1 Afghanistan Asia 1952 28.801 8425333 779.4453 6567086330 ## 2 Afghanistan Asia 1957 30.332 9240934 820.8530 7585448670 ## 3 Afghanistan Asia 1962 31.997 10267083 853.1007 8758855797 ## 4 Afghanistan Asia 1967 34.020 11537966 836.1971 9648014150 ## 5 Afghanistan Asia 1972 36.088 13079460 739.9811 9678553274 ## 6 Afghanistan Asia 1977 38.438 14880372 786.1134 11697659231 Since the number is too large to actually interpret, let’s divide it by 1 billion so that the unit of measure is in billions. We will also rename the column to represent this change.\n1 2 3 4 df$Total_GDP \u0026lt;- df$Total_GDP / 1000000000 # names(df)[names(df) == \u0026#39;old.var.name\u0026#39;] \u0026lt;- \u0026#39;new.var.name\u0026#39; names(df)[names(df) == \u0026#39;Total_GDP\u0026#39;] \u0026lt;- \u0026#39;TotalGDP_Bil\u0026#39; head(df) 1 2 3 4 5 6 7 ## country continent year lifeExp pop gdpPercap TotalGDP_Bil ## 1 Afghanistan Asia 1952 28.801 8425333 779.4453 6.567086 ## 2 Afghanistan Asia 1957 30.332 9240934 820.8530 7.585449 ## 3 Afghanistan Asia 1962 31.997 10267083 853.1007 8.758856 ## 4 Afghanistan Asia 1967 34.020 11537966 836.1971 9.648014 ## 5 Afghanistan Asia 1972 36.088 13079460 739.9811 9.678553 ## 6 Afghanistan Asia 1977 38.438 14880372 786.1134 11.697659 Note that if you were to run this multiple times, it would keep dividing by 1 billion each time, so we want to ensure it is only run once.\nNow, let’s say we want to create a new column that contains both the country and continent information together. This is a useful tool if you had information where a first name and last name were separated, or even there was a username and you wanted to create a new column by combining the username and an email extension. Let’s take a look at our example.\n1 2 df$Country_Cont \u0026lt;- paste(df$country, \u0026#39;_\u0026#39;, df$continent) head(df) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## country continent year lifeExp pop gdpPercap TotalGDP_Bil ## 1 Afghanistan Asia 1952 28.801 8425333 779.4453 6.567086 ## 2 Afghanistan Asia 1957 30.332 9240934 820.8530 7.585449 ## 3 Afghanistan Asia 1962 31.997 10267083 853.1007 8.758856 ## 4 Afghanistan Asia 1967 34.020 11537966 836.1971 9.648014 ## 5 Afghanistan Asia 1972 36.088 13079460 739.9811 9.678553 ## 6 Afghanistan Asia 1977 38.438 14880372 786.1134 11.697659 ## Country_Cont ## 1 Afghanistan _ Asia ## 2 Afghanistan _ Asia ## 3 Afghanistan _ Asia ## 4 Afghanistan _ Asia ## 5 Afghanistan _ Asia ## 6 Afghanistan _ Asia There are so many different ways to perform an operation like this.\nAs an exercise, try creating your own column using either a basic mathematical operation on an existing column, or by combining two columns, or anything else you can think of!\nSome examples:\nCreate a new column with the first three letters of the country name Create a column to show TRUE if the TotalGDP_Bil is greater than 5, FALSE if not. Combine the country name with the year. Here are the solutions to the examples provided above.\n1 2 3 # Solution to #1 df$first_3_letters \u0026lt;- substr(df$country, 1, 3) head(df) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## country continent year lifeExp pop gdpPercap TotalGDP_Bil ## 1 Afghanistan Asia 1952 28.801 8425333 779.4453 6.567086 ## 2 Afghanistan Asia 1957 30.332 9240934 820.8530 7.585449 ## 3 Afghanistan Asia 1962 31.997 10267083 853.1007 8.758856 ## 4 Afghanistan Asia 1967 34.020 11537966 836.1971 9.648014 ## 5 Afghanistan Asia 1972 36.088 13079460 739.9811 9.678553 ## 6 Afghanistan Asia 1977 38.438 14880372 786.1134 11.697659 ## Country_Cont first_3_letters ## 1 Afghanistan _ Asia Afg ## 2 Afghanistan _ Asia Afg ## 3 Afghanistan _ Asia Afg ## 4 Afghanistan _ Asia Afg ## 5 Afghanistan _ Asia Afg ## 6 Afghanistan _ Asia Afg 1 2 3 # Solution to #2 df$GDP_Bool \u0026lt;- ifelse(df$TotalGDP_Bil \u0026gt; 5, \u0026#34;TRUE\u0026#34;, \u0026#34;FALSE\u0026#34;) head(df) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## country continent year lifeExp pop gdpPercap TotalGDP_Bil ## 1 Afghanistan Asia 1952 28.801 8425333 779.4453 6.567086 ## 2 Afghanistan Asia 1957 30.332 9240934 820.8530 7.585449 ## 3 Afghanistan Asia 1962 31.997 10267083 853.1007 8.758856 ## 4 Afghanistan Asia 1967 34.020 11537966 836.1971 9.648014 ## 5 Afghanistan Asia 1972 36.088 13079460 739.9811 9.678553 ## 6 Afghanistan Asia 1977 38.438 14880372 786.1134 11.697659 ## Country_Cont first_3_letters GDP_Bool ## 1 Afghanistan _ Asia Afg TRUE ## 2 Afghanistan _ Asia Afg TRUE ## 3 Afghanistan _ Asia Afg TRUE ## 4 Afghanistan _ Asia Afg TRUE ## 5 Afghanistan _ Asia Afg TRUE ## 6 Afghanistan _ Asia Afg TRUE 1 2 3 # Solution to #3 df$Country_Year \u0026lt;- paste(df$country, \u0026#39;_\u0026#39;, df$year) head(df) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## country continent year lifeExp pop gdpPercap TotalGDP_Bil ## 1 Afghanistan Asia 1952 28.801 8425333 779.4453 6.567086 ## 2 Afghanistan Asia 1957 30.332 9240934 820.8530 7.585449 ## 3 Afghanistan Asia 1962 31.997 10267083 853.1007 8.758856 ## 4 Afghanistan Asia 1967 34.020 11537966 836.1971 9.648014 ## 5 Afghanistan Asia 1972 36.088 13079460 739.9811 9.678553 ## 6 Afghanistan Asia 1977 38.438 14880372 786.1134 11.697659 ## Country_Cont first_3_letters GDP_Bool Country_Year ## 1 Afghanistan _ Asia Afg TRUE Afghanistan _ 1952 ## 2 Afghanistan _ Asia Afg TRUE Afghanistan _ 1957 ## 3 Afghanistan _ Asia Afg TRUE Afghanistan _ 1962 ## 4 Afghanistan _ Asia Afg TRUE Afghanistan _ 1967 ## 5 Afghanistan _ Asia Afg TRUE Afghanistan _ 1972 ## 6 Afghanistan _ Asia Afg TRUE Afghanistan _ 1977 ","date":"2023-08-01T09:00:00-08:00","permalink":"http://localhost:5082/csc-data-blog/p/creating-variables-in-r/","title":"Creating Variables in R"},{"content":"Python and R are both very useful tools in academia, research, industry, and everywhere! They have a lot of similarities, but there are also many differences.\nThe purpose of this post is to help students to decide which language to learn first, according to their differences, similarities, and departmental practices.\nFirst, let\u0026rsquo;s introduce each, and talk about their main purposes.\nIntroducing Python Python is a general-purpose, object-oriented programming language. It was created in 1991, and it has a community of people who contribute to regularly updating libraries and improving efficiencies. It happens to be one of the most popular programming languages in the world. Some of the most common libraries for data-related tasks include NumPy (for arrays), Pandas (for data analysis and manipulation), and MatPlotLib (for data visualizations). Python is a powerful tool used for machine learning, deep learning, and modelling. Jupyter Notebook is a useful interface to pair with Python because it allows for clean, readable layouts to be shared with peers and users. Note that Jupyter Notebook also supports R.\nPython is commonly used in disciplines such as computer science, data science, mathematics (optimization, pure math, finance, economics, etc.), physics, engineering, the social sciences, and more. Note that this is just a general consensus. Your specific department at UBC, or your previous institution, may have done something differently.\nIntroducing R R was originally created for statistics, specializing in statistical analysis and visualization. R was created in 1993, but generally has less community support and less advancements than Python. This is one of the major complaints of users, and is one reason why many people end up turning to Python over R. Libraries and tools in R are best known for helping with tasks such as cleaning data, creating visualization, and training some machine learning and deep learning algorithms. Note that R can take significantly longer to handle machine learning algorithms than alternatives such as Python.\nR is commonly used in disciplines such as biology and math (statistics). Again, this is just a general consensus, and your specific department may use a different tool.\nSimilarities Both R and Python are open source programming languages that are maintained and supported by large communities.\nDifferences Here, we will discuss some of the differences between R and Python, which can help to decide which to start learning first.\nR is mainly used for statistical analysis, but Python is traditionally better for data wrangling. Python is more multi-purpose, and these skills can be transferable to other things, such as web development or application development. Python has a readable syntax that is easier to learn.\nLet\u0026rsquo;s take a look at this chart below to look at more differences between the two.\nR Python mainly used for statistical analysis | - better for data wrangling leans towards statistical modelling and analytics | - more multi-purpose (can transfer skills to application or web development) can perform deep statistical analysis in short code chunks | - more readable syntax data visualizations can be built from models with ggplot2 | - scalable for machine learning and data analysis, Altair has interactive and customizable visualizations only supports data formats from Excel, CSV, txt files | - supports many data formats (CSV, JSON, SQL tables, web requested data, etc.) optimized for statistical analysis of large datasets | - Pandas let\u0026rsquo;s you filter and sort data almost instantly modelling analysis requires non-core R (tidyverse used for importing, visualizing, reporting, etc.) | - standard libraries used work together well (NumPy, SciPy, scikit-learn) General Preferences In general, it is commonly considered that if you have programming experience, Python is the way to go, because it is a very easy language to learn (compared to others). If you are new to programming, Python is also considered a great learning language for beginners, and R may take some extra efforts to develop expertise.\nIf your team or supervisor prefers one language over another, it is usually easier to stick with what the people around you are using. This will allow them to help you if you run into issues. Talk to your team to find out what the majority of them prefer to use.\nOverall, R is better for statistical learning, and Python is better for machine learning, large-scaled apps, and data analysis within the web and its applications. Python and R both have great visualization tools, and you can read posts about those here as well, so visualizations shouldn\u0026rsquo;t prevent you from deciding between the two.\nR and Python are both wonderful tools to have in your toolbox for all academic and professional endeavours. When in doubt, just learn both! But if you don\u0026rsquo;t have the time, hopefully this information helps to make a more informed decision. It is also easier to learn one if you already have familiarity with the other, so feel free to keep the door open to return to learning the other option down the road.\n","date":"2023-07-26T09:00:00-08:00","permalink":"http://localhost:5082/csc-data-blog/p/python-vs-r-how-to-decide/","title":"Python vs R - How To Decide?"},{"content":"As we know, Jupyter Notebook can be used easily with our UBC login information via Syzygy. Jupyter Notebook can also be downloaded to a computer and used via Anaconda. There are many different interfaces that allow us to use Python. One of the more interesting ones to note is actually RStudio. There is a package in R called reticulate that allows us to create an RMarkdown file and use both R and Python code chunks within the same file. This is actually how this blog is able to show both R and Python content! Let\u0026rsquo;s get started on how reticulate is used in RStudio to include Python code.\nInstallation First, installation is required. To install the reticulate package in RStudio from CRAN, type the following:\n1 install.packages(\u0026#34;reticulate\u0026#34;) Python Version Next, we need to access the version of Python desired. Reticulate will, by default, find and use the version of Python via PATH. To check this, type Sys.which(\u0026quot;python\u0026quot;).\nIf you would like to change the version to something other than what is found in PATH, try the use_python() function. Here is an example of how to call the package from the library, and then use a different path to get to the desired Python version.\n1 2 library(reticulate) use_python(\u0026#34;/my/file/path/python\u0026#34;) Getting Started Now that the reticulate package has been installed, we can start using Python within an RMarkdown document.\nOnce an RMarkdown file has been started, create a new code block. Note that on a Windows, a shortcut for adding a new code block is Ctrl+alt+i. Once a new code block appears, it should look like this:\nIf you were to type code here, it would be in R. To change it to Python, simply replace the \u0026lsquo;r\u0026rsquo; with \u0026lsquo;python\u0026rsquo;. Then it will look like this:\nNow, the console below will change to show Python code once something in this block is run. You can easily switch back and forth between R and Python from code chunk to code chunk.\nYou can also see your saved variables in the environment, separated between R and Python. Note that when you do something such as import a dataset, if you wanted to perform manipulations on the dataset in both R and Python, you will have to load the dataset in using both R and Python, and it will create separate variables which do not override each other. They will stay speparate in the environment.\nTo change between R and Python environments, go to the Environment window and click the dropdown arrow. It should look like this:\nIf you would like to see some examples, take a look at the _src folder in the GitHub repo for this blog to see how each of the posts are created in both R and Python. The link to the backend content for this blog can be found here.\n","date":"2023-07-26T09:00:00-08:00","permalink":"http://localhost:5082/csc-data-blog/p/using-python-in-rstudio-with-reticulate-package/","title":"Using Python in RStudio with Reticulate Package"},{"content":"Sometimes the data frame we are working with can be very large and take a while to process. Alternatively, we could only need a portion of the information. There is a way to filter through a data frame such that only the specified information is used, which saves space and time!\nLet’s re-load in our Gapminder data:\n1 2 3 import pandas as pd url = \u0026#39;https://raw.githubusercontent.com/jstaf/gapminder/master/gapminder/gapminder.csv\u0026#39; df = pd.read_csv(url) We know that we can select just one column like this below:\n1 df[\u0026#39;country\u0026#39;] 1 2 3 4 5 6 7 8 9 10 11 12 ## 0 Afghanistan ## 1 Afghanistan ## 2 Afghanistan ## 3 Afghanistan ## 4 Afghanistan ## ... ## 1699 Zimbabwe ## 1700 Zimbabwe ## 1701 Zimbabwe ## 1702 Zimbabwe ## 1703 Zimbabwe ## Name: country, Length: 1704, dtype: object So if we wanted to create a new data frame by filtering our existing data frame for entries where the country column has the value ‘Canada’, we can do so like this:\n1 2 df_2 = df[df[\u0026#39;country\u0026#39;] == \u0026#39;Canada\u0026#39;] df_2.head() 1 2 3 4 5 6 ## country continent year lifeExp pop gdpPercap ## 240 Canada Americas 1952 68.75 14785584 11367.16112 ## 241 Canada Americas 1957 69.96 17010154 12489.95006 ## 242 Canada Americas 1962 71.30 18985849 13462.48555 ## 243 Canada Americas 1967 72.13 20819767 16076.58803 ## 244 Canada Americas 1972 72.88 22284500 18970.57086 Alternatively, if we didn’t want to create a new data frame, but we wanted to view just a subset of the data frame temporarily, we could write this:\n1 2 cols = [\u0026#39;country\u0026#39;, \u0026#39;continent\u0026#39;, \u0026#39;pop\u0026#39;] df[cols] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## country continent pop ## 0 Afghanistan Asia 8425333 ## 1 Afghanistan Asia 9240934 ## 2 Afghanistan Asia 10267083 ## 3 Afghanistan Asia 11537966 ## 4 Afghanistan Asia 13079460 ## ... ... ... ... ## 1699 Zimbabwe Africa 9216418 ## 1700 Zimbabwe Africa 10704340 ## 1701 Zimbabwe Africa 11404948 ## 1702 Zimbabwe Africa 11926563 ## 1703 Zimbabwe Africa 12311143 ## ## [1704 rows x 3 columns] We could also perform operations such as .mean(), .median(), .max(), etc. using filtering. An example of this can be found below.\n1 2 cols = [\u0026#39;lifeExp\u0026#39;, \u0026#39;pop\u0026#39;, \u0026#39;gdpPercap\u0026#39;] df[cols].mean() 1 2 3 4 ## lifeExp 5.947444e+01 ## pop 2.960121e+07 ## gdpPercap 7.215327e+03 ## dtype: float64 We can select different row \u0026amp; column combinations as well by indexing. If we wanted to select just the first entry in the first row and column, remembering that Python starts indexing at 0, it would look like this:\n1 df.iloc[0,0] 1 ## \u0026#39;Afghanistan\u0026#39; .iloc() is a function that helps us select a particular cell in the data set. We can also use it to select the first row entirely.\n1 df.iloc[0] 1 2 3 4 5 6 7 ## country Afghanistan ## continent Asia ## year 1952 ## lifeExp 28.801 ## pop 8425333 ## gdpPercap 779.445314 ## Name: 0, dtype: object We could also use it to select the first 3 rows like this:\n1 df.iloc[:3] 1 2 3 4 ## country continent year lifeExp pop gdpPercap ## 0 Afghanistan Asia 1952 28.801 8425333 779.445314 ## 1 Afghanistan Asia 1957 30.332 9240934 820.853030 ## 2 Afghanistan Asia 1962 31.997 10267083 853.100710 Which is equivalent to:\n1 df.iloc[0:3] 1 2 3 4 ## country continent year lifeExp pop gdpPercap ## 0 Afghanistan Asia 1952 28.801 8425333 779.445314 ## 1 Afghanistan Asia 1957 30.332 9240934 820.853030 ## 2 Afghanistan Asia 1962 31.997 10267083 853.100710 If we wanted to select the first column only:\n1 df.iloc[:,0] 1 2 3 4 5 6 7 8 9 10 11 12 ## 0 Afghanistan ## 1 Afghanistan ## 2 Afghanistan ## 3 Afghanistan ## 4 Afghanistan ## ... ## 1699 Zimbabwe ## 1700 Zimbabwe ## 1701 Zimbabwe ## 1702 Zimbabwe ## 1703 Zimbabwe ## Name: country, Length: 1704, dtype: object And then if we wanted to select all rows but only the second and third columns, we could do so like this:\n1 df.iloc[:, 1:3] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## continent year ## 0 Asia 1952 ## 1 Asia 1957 ## 2 Asia 1962 ## 3 Asia 1967 ## 4 Asia 1972 ## ... ... ... ## 1699 Africa 1987 ## 1700 Africa 1992 ## 1701 Africa 1997 ## 1702 Africa 2002 ## 1703 Africa 2007 ## ## [1704 rows x 2 columns] Logical Subsetting So far, we haven’t applied many conditions to our subsets, but we can. In logical subsetting, the subset defaults to returning the results where the condition is TRUE.\nRecall, Python allows us to specify several conditions:\nless than \u0026gt; greater than \u0026lt; less than or equal to \u0026lt;= greater than or equal \u0026gt;= equivalent to == not equivalent to != As well as boolean operators\nor | and \u0026amp; 1 2 df_3 = df[df[\u0026#39;pop\u0026#39;] \u0026gt; 10000000] # all variables for when population is greater than 10 million df_3.head() 1 2 3 4 5 6 ## country continent year lifeExp pop gdpPercap ## 2 Afghanistan Asia 1962 31.997 10267083 853.100710 ## 3 Afghanistan Asia 1967 34.020 11537966 836.197138 ## 4 Afghanistan Asia 1972 36.088 13079460 739.981106 ## 5 Afghanistan Asia 1977 38.438 14880372 786.113360 ## 6 Afghanistan Asia 1982 39.854 12881816 978.011439 If we wanted to filter by country name and year, without saving to a new data frame, we can do so like this:\n1 df[(df[\u0026#39;country\u0026#39;] == \u0026#39;Canada\u0026#39;) \u0026amp; (df[\u0026#39;year\u0026#39;] \u0026gt;= 2000)] 1 2 3 ## country continent year lifeExp pop gdpPercap ## 250 Canada Americas 2002 79.770 31902268 33328.96507 ## 251 Canada Americas 2007 80.653 33390141 36319.23501 Let’s say you want to know the average life expectancy in Australia for all recorded years before 2000. Try this as an exercise. To calculate this, we will first have to filter our data frame with two conditions, and then we will have to calculate the mean.\nHint: We can calculate the mean for all columns at once, and then just read off the Life Expectancy value to get the answer we are looking for.\nThere are multiple ways to solve this. See the solutions below:\n1 2 # Solution 1 df[(df[\u0026#39;country\u0026#39;] == \u0026#39;Australia\u0026#39;) \u0026amp; (df[\u0026#39;year\u0026#39;] \u0026lt; 2000)].mean(numeric_only = True) 1 2 3 4 5 ## year 1.974500e+03 ## lifeExp 7.343500e+01 ## pop 1.358108e+07 ## gdpPercap 1.746440e+04 ## dtype: float64 1 2 3 # Solution 2 filtered_df = df[(df[\u0026#39;country\u0026#39;] == \u0026#39;Australia\u0026#39;) \u0026amp; (df[\u0026#39;year\u0026#39;] \u0026lt; 2000)] filtered_df[\u0026#39;lifeExp\u0026#39;].mean() 1 ## np.float64(73.435) There are many different ways that will lead to the same answer! Try to think of another way to do it!\n","date":"2023-07-07T09:00:00-08:00","permalink":"http://localhost:5082/csc-data-blog/p/subsetting-in-python/","title":"Subsetting in Python"},{"content":"In previous categories we have looked at cleaning, summarizing, and subsetting data, with some minor calculations, but we haven’t yet looked at analyzing our data.\nPython is a very powerful tool for data analysis. Similarly to R, we can fit linear models and view graphs. First, we will look at some basic data analysis processes in Python.\nLet’s re-load in our Gapminder data:\n1 2 3 import pandas as pd url = \u0026#39;https://raw.githubusercontent.com/jstaf/gapminder/master/gapminder/gapminder.csv\u0026#39; df = pd.read_csv(url) Let’s say we want to fit a linear model to see if there is a relationship between population and time. First, let’s do this with the entire dataset with all countries.\nNote that OLS stands for ordinary least squares.\n1 2 3 4 5 6 7 import statsmodels.api as sm # define our x and y variables for clarity y = df[\u0026#39;pop\u0026#39;] x = df[\u0026#39;year\u0026#39;] x = sm.add_constant(x) model = sm.OLS(y,x).fit() model.summary() OLS Regression Results Dep. Variable: pop R-squared: 0.007 Model: OLS Adj. R-squared: 0.006 Method: Least Squares F-statistic: 11.61 Date: Tue, 08 Apr 2025 Prob (F-statistic): 0.000672 Time: 11:35:25 Log-Likelihood: -33902. No. Observations: 1704 AIC: 6.781e+04 Df Residuals: 1702 BIC: 6.782e+04 Df Model: 1 Covariance Type: nonrobust coef std err t P\u003e|t| [0.025 0.975] const -9.722e+08 2.94e+08 -3.306 0.001 -1.55e+09 -3.95e+08 year 5.061e+05 1.49e+05 3.407 0.001 2.15e+05 7.97e+05 Omnibus: 2403.823 Durbin-Watson: 0.187 Prob(Omnibus): 0.000 Jarque-Bera (JB): 438354.240 Skew: 8.286 Prob(JB): 0.00 Kurtosis: 79.807 Cond. No. 2.27e+05 Notes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 2.27e+05. This might indicate that there arestrong multicollinearity or other numerical problems. Let’s try fitting another linear model, but using only data from Africa.\n1 2 df_AF = df[df[\u0026#39;continent\u0026#39;] == \u0026#39;Africa\u0026#39;] df_AF.head() ## country continent year lifeExp pop gdpPercap ## 24 Algeria Africa 1952 43.077 9279525 2449.008185 ## 25 Algeria Africa 1957 45.685 10270856 3013.976023 ## 26 Algeria Africa 1962 48.303 11000948 2550.816880 ## 27 Algeria Africa 1967 51.407 12760499 3246.991771 ## 28 Algeria Africa 1972 54.518 14760787 4182.663766 1 2 3 4 5 y = df_AF[\u0026#39;pop\u0026#39;] x = df_AF[\u0026#39;year\u0026#39;] x = sm.add_constant(x) model1 = sm.OLS(y,x).fit() model1.summary() OLS Regression Results Dep. Variable: pop R-squared: 0.074 Model: OLS Adj. R-squared: 0.072 Method: Least Squares F-statistic: 49.66 Date: Tue, 08 Apr 2025 Prob (F-statistic): 4.87e-12 Time: 11:35:25 Log-Likelihood: -11192. No. Observations: 624 AIC: 2.239e+04 Df Residuals: 622 BIC: 2.240e+04 Df Model: 1 Covariance Type: nonrobust coef std err t P\u003e|t| [0.025 0.975] const -4.728e+08 6.85e+07 -6.902 0.000 -6.07e+08 -3.38e+08 year 2.438e+05 3.46e+04 7.047 0.000 1.76e+05 3.12e+05 Omnibus: 477.344 Durbin-Watson: 0.284 Prob(Omnibus): 0.000 Jarque-Bera (JB): 7771.738 Skew: 3.336 Prob(JB): 0.00 Kurtosis: 18.950 Cond. No. 2.27e+05 Notes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 2.27e+05. This might indicate that there arestrong multicollinearity or other numerical problems. ","date":"2023-07-06T09:00:00-08:00","permalink":"http://localhost:5082/csc-data-blog/p/basic-model-fitting-in-python/","title":"Basic Model Fitting in Python"},{"content":"Recall the URL with the country data. Let\u0026rsquo;s use this to practice some preliminary cleaning techniques.\n1 2 3 import pandas as pd url = \u0026#39;https://raw.githubusercontent.com/jstaf/gapminder/master/gapminder/gapminder.csv\u0026#39; df = pd.read_csv(url) If we wanted to check and see if there are any empty values in the data frame, we could do so like this:\n1 df.isnull().sum().sum() 1 ## np.int64(0) Above, we summed over all rows and columns to see if there were null values. if we wanted to check for null values in just one column, we could do so like this:\n1 df[\u0026#39;country\u0026#39;].isnull().sum() 1 ## np.int64(0) Renaming Columns If we wanted to look at changing the names of the columns in the data frame, whether it is because of formatting or lack of clarity in the existing titles, we can do so easily.\nLet’s say we wanted to change country to Country. In order for the changes to save, we have to update our stored variable by starting the code with df = ....\n1 2 df = df.rename({\u0026#39;country\u0026#39;: \u0026#39;Country\u0026#39;}, axis = 1) df.head() 1 2 3 4 5 6 ## Country continent year lifeExp pop gdpPercap ## 0 Afghanistan Asia 1952 28.801 8425333 779.445314 ## 1 Afghanistan Asia 1957 30.332 9240934 820.853030 ## 2 Afghanistan Asia 1962 31.997 10267083 853.100710 ## 3 Afghanistan Asia 1967 34.020 11537966 836.197138 ## 4 Afghanistan Asia 1972 36.088 13079460 739.981106 Now let’s say we want to change lifeExp to Life_Expectancy, pop to Population and gdpPercap to GDP_per_Cap.\n1 2 3 4 df = df.rename({\u0026#39;lifeExp\u0026#39;: \u0026#39;Life_Expectancy\u0026#39;, \u0026#39;pop\u0026#39;: \u0026#39;Population\u0026#39;, \u0026#39;gdpPercap\u0026#39;: \u0026#39;GDP_per_Cap\u0026#39;}, axis = 1) df.head() 1 2 3 4 5 6 ## Country continent year Life_Expectancy Population GDP_per_Cap ## 0 Afghanistan Asia 1952 28.801 8425333 779.445314 ## 1 Afghanistan Asia 1957 30.332 9240934 820.853030 ## 2 Afghanistan Asia 1962 31.997 10267083 853.100710 ## 3 Afghanistan Asia 1967 34.020 11537966 836.197138 ## 4 Afghanistan Asia 1972 36.088 13079460 739.981106 Now that our column titles are in order, we can determine the relevance of all of them. Let’s say our analysis doesn’t need the continent column, so we can get rid of it to simplify our data. To delete columns, we can do so like this:\n1 2 df = df.drop(columns = {\u0026#39;continent\u0026#39;}, axis = 1) df.head() 1 2 3 4 5 6 ## Country year Life_Expectancy Population GDP_per_Cap ## 0 Afghanistan 1952 28.801 8425333 779.445314 ## 1 Afghanistan 1957 30.332 9240934 820.853030 ## 2 Afghanistan 1962 31.997 10267083 853.100710 ## 3 Afghanistan 1967 34.020 11537966 836.197138 ## 4 Afghanistan 1972 36.088 13079460 739.981106 If we deleted the column by accident, you just have to go back and run the original line of code where we loaded the data set to start fresh and remove the changes.\nLet’s take a look at the data types in our data frame.\n1 df.dtypes 1 2 3 4 5 6 ## Country object ## year int64 ## Life_Expectancy float64 ## Population int64 ## GDP_per_Cap float64 ## dtype: object Let’s say we want to change Life_Expectancy from a float (decimal value) to an integer.\n1 2 df[\u0026#39;Life_Expectancy\u0026#39;] = df[\u0026#39;Life_Expectancy\u0026#39;].astype(int) df.dtypes 1 2 3 4 5 6 ## Country object ## year int64 ## Life_Expectancy int64 ## Population int64 ## GDP_per_Cap float64 ## dtype: object We could also convert multiple column types at once like this:\n1 2 3 df = df.astype({\u0026#34;Life_Expectancy\u0026#34;: float, \u0026#34;Population\u0026#34;: float}) df.dtypes 1 2 3 4 5 6 ## Country object ## year int64 ## Life_Expectancy float64 ## Population float64 ## GDP_per_Cap float64 ## dtype: object If we wanted to count the number of unique values in a column, we could do so like this:\n1 df.Country.unique().size 1 ## 142 ","date":"2023-06-20T09:00:00-08:00","permalink":"http://localhost:5082/csc-data-blog/p/cleaning-data-in-python/","title":"Cleaning Data in Python"},{"content":"Occasionally we need to derive variables form existing information. A good example of this is conversion between scales. If we wanted to change a measurement from metres to kilometres or weight in pounds to kilograms, we could do this simply by performing a basic operation on an entire column. In this situation, let’s start by multiplying two columns together to create a new column. Recall the GDP dataset from previous articles. Let’s say we want to know the total GDP for the country. This could be calculated by multiplying the population by the GDP_per_Cap.\nIf you are just starting here, let’s re-load in the original Gapminder data:\n1 2 3 import pandas as pd url = \u0026#39;https://raw.githubusercontent.com/jstaf/gapminder/master/gapminder/gapminder.csv\u0026#39; df = pd.read_csv(url) 1 2 df[\u0026#39;Total_GDP\u0026#39;] = df[\u0026#39;pop\u0026#39;] * df[\u0026#39;gdpPercap\u0026#39;] df.head() 1 2 3 4 5 6 ## country continent year lifeExp pop gdpPercap Total_GDP ## 0 Afghanistan Asia 1952 28.801 8425333 779.445314 6.567086e+09 ## 1 Afghanistan Asia 1957 30.332 9240934 820.853030 7.585449e+09 ## 2 Afghanistan Asia 1962 31.997 10267083 853.100710 8.758856e+09 ## 3 Afghanistan Asia 1967 34.020 11537966 836.197138 9.648014e+09 ## 4 Afghanistan Asia 1972 36.088 13079460 739.981106 9.678553e+09 Since the number is too large to actually interpret, let’s divide it by 1 billion so that the unit of measure is in billions.\n1 2 3 df[\u0026#39;Total_GDP\u0026#39;] = df[\u0026#39;Total_GDP\u0026#39;] / 1000000000 df = df.rename({\u0026#39;Total_GDP\u0026#39;: \u0026#39;TotalGDP_Bil\u0026#39;}, axis = 1) df.head() 1 2 3 4 5 6 ## country continent year lifeExp pop gdpPercap TotalGDP_Bil ## 0 Afghanistan Asia 1952 28.801 8425333 779.445314 6.567086 ## 1 Afghanistan Asia 1957 30.332 9240934 820.853030 7.585449 ## 2 Afghanistan Asia 1962 31.997 10267083 853.100710 8.758856 ## 3 Afghanistan Asia 1967 34.020 11537966 836.197138 9.648014 ## 4 Afghanistan Asia 1972 36.088 13079460 739.981106 9.678553 Note that if you were to run this multiple times, it would keep dividing by 1 billion each time, so we want to ensure it is only run once.\nNow, let’s say we want to create a new column that contains both the country and continent information together. This is a useful tool if you had information where a first name and last name were separated, or even there was a username and you wanted to create a new column by combining the username and an email extension. Let’s take a look at our example.\n1 2 df[\u0026#39;Country_Cont\u0026#39;] = df[\u0026#39;country\u0026#39;].astype(str) + \u0026#39;_\u0026#39; + df[\u0026#39;continent\u0026#39;] df.head() 1 2 3 4 5 6 7 8 ## country continent year ... gdpPercap TotalGDP_Bil Country_Cont ## 0 Afghanistan Asia 1952 ... 779.445314 6.567086 Afghanistan_Asia ## 1 Afghanistan Asia 1957 ... 820.853030 7.585449 Afghanistan_Asia ## 2 Afghanistan Asia 1962 ... 853.100710 8.758856 Afghanistan_Asia ## 3 Afghanistan Asia 1967 ... 836.197138 9.648014 Afghanistan_Asia ## 4 Afghanistan Asia 1972 ... 739.981106 9.678553 Afghanistan_Asia ## ## [5 rows x 8 columns] There are so many different ways to perform an operation like this. You could use the apply() function, the .agg() function, .map() function, and more!\nAs an exercise, try creating your own column using either a basic mathematical operation on an existing column, or by combining two columns, or anything else you can think of!\nSome examples:\nCreate a new column with the first three letters of the country name Convert the total GDP column back to the GDP per cap column by multiplying by 1 billion and then dividing by the population. Combine the country name with the year. Here are the solutions to the examples provided above.\n1 2 3 # Solution to #1 df[\u0026#39;first_3_letters\u0026#39;] = df.country.str[:3] df.head() 1 2 3 4 5 6 7 8 ## country continent year ... TotalGDP_Bil Country_Cont first_3_letters ## 0 Afghanistan Asia 1952 ... 6.567086 Afghanistan_Asia Afg ## 1 Afghanistan Asia 1957 ... 7.585449 Afghanistan_Asia Afg ## 2 Afghanistan Asia 1962 ... 8.758856 Afghanistan_Asia Afg ## 3 Afghanistan Asia 1967 ... 9.648014 Afghanistan_Asia Afg ## 4 Afghanistan Asia 1972 ... 9.678553 Afghanistan_Asia Afg ## ## [5 rows x 9 columns] 1 2 3 4 # Solution to #2 df[\u0026#39;converted_gdp_per_cap\u0026#39;] = df[\u0026#39;TotalGDP_Bil\u0026#39;] * 1000000000 df[\u0026#39;converted_gdp_per_cap\u0026#39;] = df[\u0026#39;converted_gdp_per_cap\u0026#39;] / df[\u0026#39;pop\u0026#39;] df.head() 1 2 3 4 5 6 7 8 ## country continent ... first_3_letters converted_gdp_per_cap ## 0 Afghanistan Asia ... Afg 779.445314 ## 1 Afghanistan Asia ... Afg 820.853030 ## 2 Afghanistan Asia ... Afg 853.100710 ## 3 Afghanistan Asia ... Afg 836.197138 ## 4 Afghanistan Asia ... Afg 739.981106 ## ## [5 rows x 10 columns] 1 2 3 # Solution to #3 df[\u0026#39;Country_Year\u0026#39;] = df[\u0026#39;country\u0026#39;].astype(str) + \u0026#39;_\u0026#39; + df[\u0026#39;year\u0026#39;].astype(str) df.head() 1 2 3 4 5 6 7 8 ## country continent ... converted_gdp_per_cap Country_Year ## 0 Afghanistan Asia ... 779.445314 Afghanistan_1952 ## 1 Afghanistan Asia ... 820.853030 Afghanistan_1957 ## 2 Afghanistan Asia ... 853.100710 Afghanistan_1962 ## 3 Afghanistan Asia ... 836.197138 Afghanistan_1967 ## 4 Afghanistan Asia ... 739.981106 Afghanistan_1972 ## ## [5 rows x 11 columns] ","date":"2023-06-20T09:00:00-08:00","permalink":"http://localhost:5082/csc-data-blog/p/creating-variables-in-python/","title":"Creating Variables in Python"},{"content":"You can use base Python to perform basic visualizations with a dataset. Let\u0026rsquo;s take a look at a few examples.\nLet’s re-load in our Gapminder data:\n1 2 3 import pandas as pd url = \u0026#39;https://raw.githubusercontent.com/jstaf/gapminder/master/gapminder/gapminder.csv\u0026#39; df = pd.read_csv(url) 1 2 import matplotlib.pyplot as plt plt.scatter(x = df[\u0026#39;year\u0026#39;], y = df[\u0026#39;pop\u0026#39;]) 1 2 df_AF = df[df[\u0026#39;continent\u0026#39;] == \u0026#39;Africa\u0026#39;] df_AF.head() 1 2 3 4 5 6 ## country continent year lifeExp pop gdpPercap ## 24 Algeria Africa 1952 43.077 9279525 2449.008185 ## 25 Algeria Africa 1957 45.685 10270856 3013.976023 ## 26 Algeria Africa 1962 48.303 11000948 2550.816880 ## 27 Algeria Africa 1967 51.407 12760499 3246.991771 ## 28 Algeria Africa 1972 54.518 14760787 4182.663766 1 plt.scatter(x = df_AF[\u0026#39;year\u0026#39;], y = df_AF[\u0026#39;pop\u0026#39;]) Base Python works well for simple visualizations, but Altair is a package that helps to create personalized and detailed visualizations to suit any task. See the article titled \u0026lsquo;Visualizing with Altair in Python\u0026rsquo; to learn more about it.\n","date":"2023-06-15T09:00:00-08:00","permalink":"http://localhost:5082/csc-data-blog/p/basic-visualizations-in-python/","title":"Basic Visualizations in Python"},{"content":"Functions take in data and do things with that data. We can write our own functions, but generally in Python we’ll be using functions that have already been written or built.\nSimple functions can be in base Python or in different packages. Take these base Python functions for example:\n1 sum((1,1)) 1 ## 2 1 max([1, 2, 5, 8, 3]) 1 ## 8 Calling a function requires 2 things: the function itself and any arguments the function allows us to specify – one of these arguments is the data source, but there are usually other parameters we can specify.\nIf we want to understand how the built-in functions work, let’s define our own function for addition. We start by naming it, and then defining the arguments. Here, let\u0026rsquo;s call it \u0026lsquo;add\u0026rsquo; and it will take in two numbers, so we set the arbitrary names of these arguments to x and y.\n1 2 3 4 5 def add(x,y): answer = x + y return answer add(1,1) 1 ## 2 This is cool, but unnecessary work. It’s much easier to use the built-in function that we use above. This is why packages are so useful. Other people have spent time defining functions to do typical tasks, so we can use the existing work rather than duplicating effort.\nNote that there may be limitations or unexpected behaviours to a function. What would happen if we added x+y in this function?\n1 add(\u0026#39;x\u0026#39;,\u0026#39;y\u0026#39;) 1 ## \u0026#39;xy\u0026#39; This is something to consider when creating functions! It may not always be used for its intended use.\nIf we wanted to create a function to reverse a number, we could do that first by defining the name of our function, as well as how many arguments it requires. At the end, a function will need to do something either by using the command return or print or something else, otherwise it serves no purpose.\nWith this function, if we input the number 12345, we would expect it to return 54321.\n1 2 3 4 5 6 7 8 9 10 def reverse_num(number): num = number reversed_num = 0 while num != 0: digit = num % 10 reversed_num = reversed_num * 10 + digit num //= 10 print(\u0026#34;Reversed Number: \u0026#34; + str(reversed_num)) reverse_num(12345) 1 ## Reversed Number: 54321 It works! Can you think of any limitations or unexpected uses for this function?\n","date":"2023-06-15T09:00:00-08:00","permalink":"http://localhost:5082/csc-data-blog/p/defining-functions-in-python/","title":"Defining Functions in Python"},{"content":"A dictionary lists key-value pairs, which could also be thought of as associated values where a key matches to the associated value. Let\u0026rsquo;s look at a few examples.\n1 2 3 # Dictionary - mapping between values house = {\u0026#39;bedrooms\u0026#39;: 3, \u0026#39;bathrooms\u0026#39;: 2, \u0026#39;city\u0026#39;: \u0026#39;Kelowna\u0026#39;, \u0026#39;price\u0026#39;: 250000} 1 house[\u0026#39;price\u0026#39;] 1 ## 250000 1 2 course = {\u0026#39;Data Science\u0026#39;: [\u0026#39;DATA100\u0026#39;, \u0026#39;DATA200\u0026#39;, \u0026#39;DATA300\u0026#39;], \u0026#39;Science\u0026#39;: [\u0026#39;SCIENCE100\u0026#39;, \u0026#39;SCIENCE200\u0026#39;, \u0026#39;SCIENCE300\u0026#39;]} 1 course[\u0026#39;Data Science\u0026#39;] 1 ## [\u0026#39;DATA100\u0026#39;, \u0026#39;DATA200\u0026#39;, \u0026#39;DATA300\u0026#39;] What if we wanted to turn the following information into a dictionary.\nName ID Campus Courses Dan 12345678 Okanagan DATA100, ENGL100, HIST100, CHEM100 This is how we would do this:\n1 2 3 4 5 student = {\u0026#39;Name\u0026#39; : \u0026#39;Dan\u0026#39;, \u0026#39;ID\u0026#39; : 12345678, \u0026#39;Campus\u0026#39; : \u0026#39;Okanagan\u0026#39;, \u0026#39;Courses\u0026#39;: [\u0026#39;DATA100\u0026#39;, \u0026#39;ENGL100\u0026#39;, \u0026#39;HIST100\u0026#39;, \u0026#39;CHEM100\u0026#39;]} student 1 ## {\u0026#39;Name\u0026#39;: \u0026#39;Dan\u0026#39;, \u0026#39;ID\u0026#39;: 12345678, \u0026#39;Campus\u0026#39;: \u0026#39;Okanagan\u0026#39;, \u0026#39;Courses\u0026#39;: [\u0026#39;DATA100\u0026#39;, \u0026#39;ENGL100\u0026#39;, \u0026#39;HIST100\u0026#39;, \u0026#39;CHEM100\u0026#39;]} ","date":"2023-06-15T09:00:00-08:00","permalink":"http://localhost:5082/csc-data-blog/p/dictionaries-in-python/","title":"Dictionaries in Python"},{"content":"Lists are an important tool used in Python. Lists can contain elements of mixed types as well. Let\u0026rsquo;s look at a few examples.\n1 2 3 # List list1 = [] list1 1 ## [] 1 2 list2 = [1, \u0026#39;UBC\u0026#39;, 100] list2 1 ## [1, \u0026#39;UBC\u0026#39;, 100] Now let\u0026rsquo;s see a few operations we can perform on a list.\n1 len(list1) 1 ## 0 1 len(list2) 1 ## 3 1 2 # Indexing Example - we will talk more about this later on list2[2] 1 ## 100 Note - Python indexing starts at zero, so the first element in the list is 0, the second is 1, and the third is 2, so if we actually wanted the second element, we would have to do this:\n1 list2[1] 1 ## \u0026#39;UBC\u0026#39; 1 type(list2) 1 ## \u0026lt;class \u0026#39;list\u0026#39;\u0026gt; 1 type(list2[1]) 1 ## \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; As we mentioned earlier, a main difference between lists and tuples is that lists are mutable. Mutable means that elements in a list can be appended, changed, or deleted.\n1 2 3 mixed_list = [1, \u0026#39;UBC\u0026#39;, 5.0, \u0026#39;1\u0026#39;] mixed_list.append(\u0026#39;New\u0026#39;) mixed_list 1 ## [1, \u0026#39;UBC\u0026#39;, 5.0, \u0026#39;1\u0026#39;, \u0026#39;New\u0026#39;] If we decided that we wanted to replace \u0026lsquo;New\u0026rsquo; with 4, we could do so like this:\n1 2 mixed_list[4]= 4 mixed_list 1 ## [1, \u0026#39;UBC\u0026#39;, 5.0, \u0026#39;1\u0026#39;, 4] If we wanted to remove elements from the list, we could do so like this:\n1 2 mixed_list.remove(\u0026#39;UBC\u0026#39;) mixed_list 1 ## [1, 5.0, \u0026#39;1\u0026#39;, 4] Note that this will only remove the first occurrence, if this happened to be in the list more than once. It would also call an error message if the element doesn\u0026rsquo;t exist.\nIf we wanted to remove it based on the index of the list:\n1 2 del mixed_list[0] mixed_list 1 ## [5.0, \u0026#39;1\u0026#39;, 4] ","date":"2023-06-15T09:00:00-08:00","permalink":"http://localhost:5082/csc-data-blog/p/lists-in-python/","title":"Lists in Python"},{"content":"Loops are a common tool used in Python to help users iterate through lists, or perform the same operation numerous times without added efforts.\nLet\u0026rsquo;s look at a few examples:\n1 2 3 4 5 # Loops for n in [0, 1, 5, 2, -5]: # this is inside the loop print(\u0026#34;The number is\u0026#34;, n, \u0026#34;and its squared value is\u0026#34;, n**2) 1 2 3 4 5 ## The number is 0 and its squared value is 0 ## The number is 1 and its squared value is 1 ## The number is 5 and its squared value is 25 ## The number is 2 and its squared value is 4 ## The number is -5 and its squared value is 25 1 # this is outside the loop 1 2 3 4 ## Loop s = \u0026#34;Python\u0026#34; for c in s: print(c + \u0026#34;!\u0026#34;) 1 2 3 4 5 6 ## P! ## y! ## t! ## h! ## o! ## n! 1 2 3 # range(10) sets values 0-9, because recall Python starts at 0, not 1 for i in range(10): print(i) 1 2 3 4 5 6 7 8 9 10 ## 0 ## 1 ## 2 ## 3 ## 4 ## 5 ## 6 ## 7 ## 8 ## 9 This is equivalent to writing:\n1 2 for i in range(0,10): print(i) 1 2 3 4 5 6 7 8 9 10 ## 0 ## 1 ## 2 ## 3 ## 4 ## 5 ## 6 ## 7 ## 8 ## 9 If we wanted it to start at 1 and go to 10 (inclusive), we would write:\n1 2 for i in range(1,11): print(i) 1 2 3 4 5 6 7 8 9 10 ## 1 ## 2 ## 3 ## 4 ## 5 ## 6 ## 7 ## 8 ## 9 ## 10 Other examples:\n1 2 3 #(start,end,increments) for i in range(0,101,10): print(i) 1 2 3 4 5 6 7 8 9 10 11 ## 0 ## 10 ## 20 ## 30 ## 40 ## 50 ## 60 ## 70 ## 80 ## 90 ## 100 1 2 3 4 n = 3 while n \u0026gt; 0: print(n) n = n - 1 1 2 3 ## 3 ## 2 ## 1 1 print(\u0026#34;Smile!\u0026#34;) 1 ## Smile! ","date":"2023-06-15T09:00:00-08:00","permalink":"http://localhost:5082/csc-data-blog/p/loops-in-python/","title":"Loops in Python"},{"content":"Since Python is so widely used, there are many people who contribute to continuously improving and developing it. Let’s imagine Python as a base version. It can do basic calculations, but it requires extra efforts to do more complicated things. People have created extras or add-ons to help create shortcuts for more complicated specific tasks or functions. These ‘add-ons’ are called modules. A module allows us to use already defined classes, functions, variables, and more. Packages are a collection of similar modules. These modules are stored together in a package to help with storage and ease of use.\nPackages can be imported to use to help make things easier. For example, there is a Python package called math that helps with basic mathematical operations.\nTo use a package or specific module within it, we use the import command. The format of these commands is as followed:\nimport package.subpackage.modulename\nWe can also nickname the packages to make it easier when we use them. This is done using the as statement. An example of this is import pandas as pd. By doing this, whenever a function from pandas is used, instead of having to type pandas.function_name, we can type pd.function_name. This may not seem like a big difference, but when it is used repeatedly, this can save a lot of time and effort.\nA few of the main packages used in python include:\nPackage Name Usage Standard Import Command NumPy Used for arrays, matricies and mathematical functions import numpy as np pandas Used with data frames import pandas as pd matplotlib Typically used for plotting functions from matplotlib import pyplot as plt altair More advanced plotting options import altair as alt SciPy Used for scientific and technical computing import scipy There are many other packages available, and you can even create your own as well! To see a list of the top packages available in Python, visit this link.\n","date":"2023-06-15T09:00:00-08:00","permalink":"http://localhost:5082/csc-data-blog/p/packages-in-python/","title":"Packages in Python"},{"content":"When we talk about data, we can talk about data types, data classes, and data structures.\nData types are fundamental building blocks for storing information. Below, we can see a chart of all of the datatypes in Python.\nEnglish name Type name Description Example integer int positive/negative whole numbers 13 floating point number float real number in decimal form 3.1415 boolean bool true or false True string str text \u0026ldquo;Do you like Python?\u0026quot; list list a collection of objects - mutable \u0026amp; ordered [\u0026lsquo;Hi\u0026rsquo;,\u0026lsquo;Hello\u0026rsquo;,\u0026lsquo;Hola\u0026rsquo;] tuple tuple a collection of objects - immutable \u0026amp; ordered (\u0026lsquo;Tuesday\u0026rsquo;,3,14,2023) dictionary dict mapping of key-value pairs {\u0026rsquo;name\u0026rsquo;:\u0026lsquo;Madison\u0026rsquo;,\u0026lsquo;Program\u0026rsquo;:\u0026lsquo;Data Science\u0026rsquo;,\u0026lsquo;Age\u0026rsquo;:23} none NoneType represents no value None Note that character data, also known as strings, are always wrapped in “quotation marks”. You can use single quotes like ‘this’ or double quotes like “this”. As long as you use two of the same, it doesn’t matter which you use!\nWe can use the function type() to view the datatype of the stored value in a variable. Let’s explore a few examples of this.\n1 2 3 # Datatype x = 1 + 1 type(x) 1 ## \u0026lt;class \u0026#39;int\u0026#39;\u0026gt; 1 2 x = 2.0 type(x) 1 ## \u0026lt;class \u0026#39;float\u0026#39;\u0026gt; 1 2 3 # String string = \u0026#39;Okanagan\u0026#39; print(string) 1 ## Okanagan 1 type(x) 1 ## \u0026lt;class \u0026#39;float\u0026#39;\u0026gt; Comparison Operators We can compare objects (or variables) using comparison operators. The result is a Boolean value. Recall from above that a Boolean gives either a True or False.\nFirst, let\u0026rsquo;s look at a table of the different comparison operators.\nComparison Operators Operator Description x == y is x equal to y? x != y is x not equal to y? x \u0026gt; y is x greater than y? x \u0026gt;= y is x greater than or equal to y? x \u0026lt; y is x less than y? x \u0026lt;= y is x less than or equal to y? x is y is x the same object as y? x and y are x and y both true? x or y is at least one of x and y true? not x is x false? Now, let\u0026rsquo;s look at a few examples.\n1 5 \u0026lt; 7 1 ## True 1 2 3 five = 5 seven = 7 five \u0026lt; seven 1 ## True 1 5.0 == 5 1 ## True 1 5.0 == \u0026#39;5\u0026#39; 1 ## False Lists Lists are an important tool used in Python. Lists can contain elements of mixed types as well. Let\u0026rsquo;s look at a few examples.\n1 2 3 # List list1 = [] list1 1 ## [] 1 2 list2 = [1, \u0026#39;UBC\u0026#39;, 100] list2 1 ## [1, \u0026#39;UBC\u0026#39;, 100] Now let\u0026rsquo;s see a few operations we can perform on a list.\n1 len(list1) 1 ## 0 1 len(list2) 1 ## 3 1 2 # Indexing Example - we will talk more about this later on list2[2] 1 ## 100 Note - Python indexing starts at zero, so the first element in the list is 0, the second is 1, and the third is 2, so if we actually wanted the second element, we would have to do this:\n1 list2[1] 1 ## \u0026#39;UBC\u0026#39; 1 type(list2) 1 ## \u0026lt;class \u0026#39;list\u0026#39;\u0026gt; 1 type(list2[1]) 1 ## \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; As we mentioned earlier, a main difference between lists and tuples is that lists are mutable. Mutable means that elements in a list can be appended, changed, or deleted.\n1 2 3 mixed_list = [1, \u0026#39;UBC\u0026#39;, 5.0, \u0026#39;1\u0026#39;] mixed_list.append(\u0026#39;New\u0026#39;) mixed_list 1 ## [1, \u0026#39;UBC\u0026#39;, 5.0, \u0026#39;1\u0026#39;, \u0026#39;New\u0026#39;] If we decided that we wanted to replace \u0026lsquo;New\u0026rsquo; with 4, we could do so like this:\n1 2 mixed_list[4]= 4 mixed_list 1 ## [1, \u0026#39;UBC\u0026#39;, 5.0, \u0026#39;1\u0026#39;, 4] If we wanted to remove elements from the list, we could do so like this:\n1 2 mixed_list.remove(\u0026#39;UBC\u0026#39;) mixed_list 1 ## [1, 5.0, \u0026#39;1\u0026#39;, 4] Note that this will only remove the first occurrence, if this happened to be in the list more than once. It would also call an error message if the element doesn\u0026rsquo;t exist.\nIf we wanted to remove it based on the index of the list:\n1 2 del mixed_list[0] mixed_list 1 ## [5.0, \u0026#39;1\u0026#39;, 4] Dictionaries A dictionary lists key-value pairs, which could also be thought of as associated values where a key matches to the associated value. Let\u0026rsquo;s look at a few examples.\n1 2 3 # Dictionary - mapping between values house = {\u0026#39;bedrooms\u0026#39;: 3, \u0026#39;bathrooms\u0026#39;: 2, \u0026#39;city\u0026#39;: \u0026#39;Kelowna\u0026#39;, \u0026#39;price\u0026#39;: 250000} 1 house[\u0026#39;price\u0026#39;] 1 ## 250000 1 2 course = {\u0026#39;Data Science\u0026#39;: [\u0026#39;DATA100\u0026#39;, \u0026#39;DATA200\u0026#39;, \u0026#39;DATA300\u0026#39;], \u0026#39;Science\u0026#39;: [\u0026#39;SCIENCE100\u0026#39;, \u0026#39;SCIENCE200\u0026#39;, \u0026#39;SCIENCE300\u0026#39;]} 1 course[\u0026#39;Data Science\u0026#39;] 1 ## [\u0026#39;DATA100\u0026#39;, \u0026#39;DATA200\u0026#39;, \u0026#39;DATA300\u0026#39;] Loops Loops are a common tool used in Python to help users iterate through lists, or perform the same operation numerous times without added efforts.\nLet\u0026rsquo;s look at a few examples:\n1 2 3 4 5 # Loops for n in [0, 1, 5, 2, -5]: # this is inside the loop print(\u0026#34;The number is\u0026#34;, n, \u0026#34;and its squared value is\u0026#34;, n**2) 1 2 3 4 5 ## The number is 0 and its squared value is 0 ## The number is 1 and its squared value is 1 ## The number is 5 and its squared value is 25 ## The number is 2 and its squared value is 4 ## The number is -5 and its squared value is 25 1 # this is outside the loop 1 2 3 4 ## Loop s = \u0026#34;Python\u0026#34; for c in s: print(c + \u0026#34;!\u0026#34;) 1 2 3 4 5 6 ## P! ## y! ## t! ## h! ## o! ## n! 1 2 3 # range(10) sets values 0-9, because recall Python starts at 0, not 1 for i in range(10): print(i) 1 2 3 4 5 6 7 8 9 10 ## 0 ## 1 ## 2 ## 3 ## 4 ## 5 ## 6 ## 7 ## 8 ## 9 This is equivalent to writing:\n1 2 for i in range(0,10): print(i) 1 2 3 4 5 6 7 8 9 10 ## 0 ## 1 ## 2 ## 3 ## 4 ## 5 ## 6 ## 7 ## 8 ## 9 If we wanted it to start at 1 and go to 10 (inclusive), we would write:\n1 2 for i in range(1,11): print(i) 1 2 3 4 5 6 7 8 9 10 ## 1 ## 2 ## 3 ## 4 ## 5 ## 6 ## 7 ## 8 ## 9 ## 10 Other examples:\n1 2 3 #(start,end,increments) for i in range(0,101,10): print(i) 1 2 3 4 5 6 7 8 9 10 11 ## 0 ## 10 ## 20 ## 30 ## 40 ## 50 ## 60 ## 70 ## 80 ## 90 ## 100 1 2 3 4 n = 3 while n \u0026gt; 0: print(n) n = n - 1 1 2 3 ## 3 ## 2 ## 1 1 print(\u0026#34;Smile!\u0026#34;) 1 ## Smile! Data Frames A data frame essentially functions as a series of connected vectors or lists, where each vector or list is a column. In this sense a data frame is also a special kind of list.\nIn a data frame, all vectors need to be of the same length. And while each vector must hold the same data type, not all vectors need to be of the same data type. Data frames also allow us to apply column names.\nLet\u0026rsquo;s look at an example of creating a dataframe from two lists.\n1 2 3 4 5 6 7 8 letter_list = [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;f\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;h\u0026#39;, \u0026#39;i\u0026#39;] number_list = [1, 2, 3, 4, 5, 6, 7, 8, 9] import pandas as pd df = pd.DataFrame( {\u0026#39;Letters\u0026#39;: letter_list, \u0026#39;Numbers\u0026#39;: number_list}) df 1 2 3 4 5 6 7 8 9 10 ## Letters Numbers ## 0 a 1 ## 1 b 2 ## 2 c 3 ## 3 d 4 ## 4 e 5 ## 5 f 6 ## 6 g 7 ## 7 h 8 ## 8 i 9 Alternatively, you could write a data frame directly like this:\n1 2 3 4 df1 = pd.DataFrame({\u0026#39;x\u0026#39; : [1., 2., 3., 4.], \u0026#39;y\u0026#39; : [4., 3., 2., 1.], \u0026#39;z\u0026#39; : [1, 2, 3, 4]}) df1 1 2 3 4 5 ## x y z ## 0 1.0 4.0 1 ## 1 2.0 3.0 2 ## 2 3.0 2.0 3 ## 3 4.0 1.0 4 Data frames are a VERY powerful data type in Python. We will spend most of our remaining time working with data frames because there is so much to learn about.\n","date":"2023-06-15T09:00:00-08:00","permalink":"http://localhost:5082/csc-data-blog/p/python-data-types/","title":"Python Data Types"},{"content":"The Pandas package is the go-to package in Python for data frames and data set analysis.\nLet\u0026rsquo;s load in a dataset from a URL and then explore the data. We can enter the URL to the data set and save it as a variable so that it is easily accessible. Since the data set is a .csv file, we can use the pandas function read_csv() to save the data into a data frame!\n1 2 3 import pandas as pd # we nickname it pd to save time when calling it later url = \u0026#39;https://raw.githubusercontent.com/jstaf/gapminder/master/gapminder/gapminder.csv\u0026#39; df = pd.read_csv(url) We name the data set df which is short for data frame. This is just a common name used for naming data, but any name could be used depending on preference.\nThe head() function is often used to show the first few rows of a data frame. The default is the first five rows, but you can enter a different value inside the brackets to get the first n number of rows.\n1 df.head() 1 2 3 4 5 6 ## country continent year lifeExp pop gdpPercap ## 0 Afghanistan Asia 1952 28.801 8425333 779.445314 ## 1 Afghanistan Asia 1957 30.332 9240934 820.853030 ## 2 Afghanistan Asia 1962 31.997 10267083 853.100710 ## 3 Afghanistan Asia 1967 34.020 11537966 836.197138 ## 4 Afghanistan Asia 1972 36.088 13079460 739.981106 But if we only wanted to see the first 3 rows\u0026hellip;\n1 df.head(3) 1 2 3 4 ## country continent year lifeExp pop gdpPercap ## 0 Afghanistan Asia 1952 28.801 8425333 779.445314 ## 1 Afghanistan Asia 1957 30.332 9240934 820.853030 ## 2 Afghanistan Asia 1962 31.997 10267083 853.100710 Remember, Python starts at 0 and not 1, so the first row of data is technically zero! Also, notice that pandas noticed that the first row of the csv file was header names, so it automatically created column titles.\nWe can also do this for the last few rows of the data set, know as the tail.\n1 df.tail() 1 2 3 4 5 6 ## country continent year lifeExp pop gdpPercap ## 1699 Zimbabwe Africa 1987 62.351 9216418 706.157306 ## 1700 Zimbabwe Africa 1992 60.377 10704340 693.420786 ## 1701 Zimbabwe Africa 1997 46.809 11404948 792.449960 ## 1702 Zimbabwe Africa 2002 39.989 11926563 672.038623 ## 1703 Zimbabwe Africa 2007 43.487 12311143 469.709298 We can now explore the data a bit more, looking at the data types and data structures.\nWe can use the .info() function to get a summary of data frame information, such as the count of null values, data types, etc. Take a look below.\n1 df.info() 1 2 3 4 5 6 7 8 9 10 11 12 13 ## \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; ## RangeIndex: 1704 entries, 0 to 1703 ## Data columns (total 6 columns): ## # Column Non-Null Count Dtype ## --- ------ -------------- ----- ## 0 country 1704 non-null object ## 1 continent 1704 non-null object ## 2 year 1704 non-null int64 ## 3 lifeExp 1704 non-null float64 ## 4 pop 1704 non-null int64 ## 5 gdpPercap 1704 non-null float64 ## dtypes: float64(2), int64(2), object(2) ## memory usage: 80.0+ KB We can also quickly see summary statistics using the .describe() function.\n1 df.describe() 1 2 3 4 5 6 7 8 9 ## year lifeExp pop gdpPercap ## count 1704.00000 1704.000000 1.704000e+03 1704.000000 ## mean 1979.50000 59.474439 2.960121e+07 7215.327081 ## std 17.26533 12.917107 1.061579e+08 9857.454543 ## min 1952.00000 23.599000 6.001100e+04 241.165876 ## 25% 1965.75000 48.198000 2.793664e+06 1202.060309 ## 50% 1979.50000 60.712500 7.023596e+06 3531.846988 ## 75% 1993.25000 70.845500 1.958522e+07 9325.462346 ## max 2007.00000 82.603000 1.318683e+09 113523.132900 If we wanted to know the number of unique values of a specific column, we could use the value_counts() function.\n1 df.country.value_counts() 1 2 3 4 5 6 7 8 9 10 11 12 13 ## country ## Afghanistan 12 ## Pakistan 12 ## New Zealand 12 ## Nicaragua 12 ## Niger 12 ## .. ## Eritrea 12 ## Equatorial Guinea 12 ## El Salvador 12 ## Egypt 12 ## Zimbabwe 12 ## Name: count, Length: 142, dtype: int64 This shows that there are 12 occurrences of each unique country value.\nIf we wanted to know the number of countries recorded for each year, how would we find that information?\nWe can do so like this:\n1 df.year.value_counts() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## year ## 1952 142 ## 1957 142 ## 1962 142 ## 1967 142 ## 1972 142 ## 1977 142 ## 1982 142 ## 1987 142 ## 1992 142 ## 1997 142 ## 2002 142 ## 2007 142 ## Name: count, dtype: int64 If we wanted to find specific values, let’s say the maximum values, we could use the function .max().\n1 df.max() 1 2 3 4 5 6 7 ## country Zimbabwe ## continent Oceania ## year 2007 ## lifeExp 82.603 ## pop 1318683096 ## gdpPercap 113523.1329 ## dtype: object Other similar functions include .min(), .mean(), and .median().\nIf we wanted to group by specific data, let’s say country, and then apply an aggregate function, we could do it like this:\n1 df.groupby(\u0026#39;country\u0026#39;).max() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ## continent year lifeExp pop gdpPercap ## country ## Afghanistan Asia 2007 43.828 31889923 978.011439 ## Albania Europe 2007 76.423 3600523 5937.029526 ## Algeria Africa 2007 72.301 33333216 6223.367465 ## Angola Africa 2007 42.731 12420476 5522.776375 ## Argentina Americas 2007 75.320 40301927 12779.379640 ## ... ... ... ... ... ... ## Vietnam Asia 2007 74.249 85262356 2441.576404 ## West Bank and Gaza Asia 2007 73.422 4018332 7110.667619 ## Yemen, Rep. Asia 2007 62.698 22211743 2280.769906 ## Zambia Africa 2007 51.821 11746035 1777.077318 ## Zimbabwe Africa 2007 62.351 12311143 799.362176 ## ## [142 rows x 5 columns] We can also group by multiple columns.\n1 df.groupby([\u0026#39;country\u0026#39;, \u0026#39;year\u0026#39;]).max() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ## continent lifeExp pop gdpPercap ## country year ## Afghanistan 1952 Asia 28.801 8425333 779.445314 ## 1957 Asia 30.332 9240934 820.853030 ## 1962 Asia 31.997 10267083 853.100710 ## 1967 Asia 34.020 11537966 836.197138 ## 1972 Asia 36.088 13079460 739.981106 ## ... ... ... ... ... ## Zimbabwe 1987 Africa 62.351 9216418 706.157306 ## 1992 Africa 60.377 10704340 693.420786 ## 1997 Africa 46.809 11404948 792.449960 ## 2002 Africa 39.989 11926563 672.038623 ## 2007 Africa 43.487 12311143 469.709298 ## ## [1704 rows x 4 columns] If we wanted to know the number of countries in each continent recorded for each year, how would we find that information?\n1 df.groupby([\u0026#39;year\u0026#39;]).continent.value_counts() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 ## year continent ## 1952 Africa 52 ## Asia 33 ## Europe 30 ## Americas 25 ## Oceania 2 ## 1957 Africa 52 ## Asia 33 ## Europe 30 ## Americas 25 ## Oceania 2 ## 1962 Africa 52 ## Asia 33 ## Europe 30 ## Americas 25 ## Oceania 2 ## 1967 Africa 52 ## Asia 33 ## Europe 30 ## Americas 25 ## Oceania 2 ## 1972 Africa 52 ## Asia 33 ## Europe 30 ## Americas 25 ## Oceania 2 ## 1977 Africa 52 ## Asia 33 ## Europe 30 ## Americas 25 ## Oceania 2 ## 1982 Africa 52 ## Asia 33 ## Europe 30 ## Americas 25 ## Oceania 2 ## 1987 Africa 52 ## Asia 33 ## Europe 30 ## Americas 25 ## Oceania 2 ## 1992 Africa 52 ## Asia 33 ## Europe 30 ## Americas 25 ## Oceania 2 ## 1997 Africa 52 ## Asia 33 ## Europe 30 ## Americas 25 ## Oceania 2 ## 2002 Africa 52 ## Asia 33 ## Europe 30 ## Americas 25 ## Oceania 2 ## 2007 Africa 52 ## Asia 33 ## Europe 30 ## Americas 25 ## Oceania 2 ## Name: count, dtype: int64 Notice that we did not change anything in our original data that we loaded in all of the code blocks above. We just simply viewed the data in different ways. If you wanted to clean the data or create a permanent copy of the grouped values, you would have to save it as a variable by typing a name on the left hand side, the equals sign, and then the action you want saved on the right hand side. For example:\n1 df2 = df.groupby([\u0026#39;year\u0026#39;]).continent.value_counts() Here we just did the same thing as earlier, but saved it to a new variable called df2.\n","date":"2023-06-15T09:00:00-08:00","permalink":"http://localhost:5082/csc-data-blog/p/viewing-data-in-python/","title":"Viewing Data in Python"},{"content":"Matplotlib is a standard package used in Python for plotting. This is a basic plotting package, but does have limitations.\nIf you are familiar with R, you may have used ggplot2 before. In Python, the equivalent package would be Altair. We will take a look at some basic Altair plotting tools.\nLet’s switch things up now and use a different data set to visualize. Like R, Python has a few built-in data sets. A popular R dataset, iris, can also be found in Python. Let’s load the iris data set here.\n1 2 3 import statsmodels.api as sm iris = sm.datasets.get_rdataset(\u0026#39;iris\u0026#39;).data iris ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 0 5.1 3.5 1.4 0.2 setosa ## 1 4.9 3.0 1.4 0.2 setosa ## 2 4.7 3.2 1.3 0.2 setosa ## 3 4.6 3.1 1.5 0.2 setosa ## 4 5.0 3.6 1.4 0.2 setosa ## .. ... ... ... ... ... ## 145 6.7 3.0 5.2 2.3 virginica ## 146 6.3 2.5 5.0 1.9 virginica ## 147 6.5 3.0 5.2 2.0 virginica ## 148 6.2 3.4 5.4 2.3 virginica ## 149 5.9 3.0 5.1 1.8 virginica ## ## [150 rows x 5 columns] Let’s first start by looking at matplotlib.\nWe will take a look at a scatterplot of the first two columns in iris.\n1 2 import matplotlib.pyplot as plt plt.scatter(x = iris[\u0026#39;Sepal.Length\u0026#39;], y = iris[\u0026#39;Sepal.Width\u0026#39;]) How about a boxplot?\n1 2 new_data = iris[[\u0026#34;Sepal.Length\u0026#34;, \u0026#34;Sepal.Width\u0026#34;, \u0026#34;Petal.Length\u0026#34;, \u0026#34;Petal.Width\u0026#34;]] new_data.boxplot() If we wanted to add a title and axis labels to the plot:\n1 2 3 4 new_data.boxplot() plt.title(\u0026#34;Sample Boxplot\u0026#34;) plt.xlabel(\u0026#34;Measurements\u0026#34;) plt.ylabel(\u0026#34;Values\u0026#34;) We can also use the functions plt.hist() and plt.bar() to generate histograms and boxplots, respectively.\nNow, let’s take a look at a few of Altair’s functions.\nIn order to use Altair, we have to change the column names because it does not support the functionality with Column.Name.\n1 2 3 4 5 6 # rename columns iris = iris.rename(columns={\u0026#39;Sepal.Length\u0026#39;: \u0026#39;SepalLength\u0026#39;, \u0026#39;Sepal.Width\u0026#39;: \u0026#39;SepalWidth\u0026#39;, \u0026#39;Petal.Length\u0026#39;: \u0026#39;PetalLength\u0026#39;, \u0026#39;Petal.Width\u0026#39;: \u0026#39;PetalWidth\u0026#39;}) iris ## SepalLength SepalWidth PetalLength PetalWidth Species ## 0 5.1 3.5 1.4 0.2 setosa ## 1 4.9 3.0 1.4 0.2 setosa ## 2 4.7 3.2 1.3 0.2 setosa ## 3 4.6 3.1 1.5 0.2 setosa ## 4 5.0 3.6 1.4 0.2 setosa ## .. ... ... ... ... ... ## 145 6.7 3.0 5.2 2.3 virginica ## 146 6.3 2.5 5.0 1.9 virginica ## 147 6.5 3.0 5.2 2.0 virginica ## 148 6.2 3.4 5.4 2.3 virginica ## 149 5.9 3.0 5.1 1.8 virginica ## ## [150 rows x 5 columns] Let’s check the data types.\n1 iris.dtypes ## SepalLength float64 ## SepalWidth float64 ## PetalLength float64 ## PetalWidth float64 ## Species object ## dtype: object Now that we’re ready, let’s view a scatter plot of the first two columns in iris.\n1 2 3 4 5 import altair as alt alt.Chart(iris).mark_point().encode( x = \u0026#39;SepalLength\u0026#39;, y = \u0026#39;SepalWidth\u0026#39; ) \u003c!DOCTYPE html\u003e If we wanted to view this same scatterplot but also distinguish by colour, we could add in one small line at the end. Also, let’s add some axis titles and change the scale to reduce the white space.\n1 2 3 4 5 alt.Chart(iris, title=\u0026#34;Comparing Sepal Length to Sepal Width\u0026#34;).mark_point().encode( x = alt.X(\u0026#39;SepalLength\u0026#39;, title = \u0026#39;Sepal Length\u0026#39;, scale = alt.Scale(domain = (4,9))), y = alt.Y(\u0026#39;SepalWidth\u0026#39;, title = \u0026#39;Sepal Width\u0026#39;, scale = alt.Scale(domain = (1.5,4.5))), color = \u0026#39;Species\u0026#39; ) \u003c!DOCTYPE html\u003e Unfortunately, Altair only accepts the US spelling of colour!\nWe see that the red and orange are a bit hard to distinguish. We can add in different shapes to help distinguish between species.\n1 2 3 4 5 6 alt.Chart(iris, title=\u0026#34;Comparing Sepal Length to Sepal Width\u0026#34;).mark_point().encode( x = alt.X(\u0026#39;SepalLength\u0026#39;, title = \u0026#39;Sepal Length\u0026#39;, scale = alt.Scale(domain = (4,9))), y = alt.Y(\u0026#39;SepalWidth\u0026#39;, title = \u0026#39;Sepal Width\u0026#39;, scale = alt.Scale(domain = (1.5,4.5))), color = \u0026#39;Species\u0026#39;, shape = \u0026#39;Species\u0026#39; ) \u003c!DOCTYPE html\u003e Tooltips There is a feature in Altair called a tooltip that allows users to interact with the plot.\nLet’s add a tooltip to the scatterplot above to see how it looks.\n1 2 3 4 5 6 alt.Chart(iris, title=\u0026#34;Comparing Sepal Length to Sepal Width\u0026#34;).mark_point().encode( x = alt.X(\u0026#39;SepalLength\u0026#39;, title = \u0026#39;Sepal Length\u0026#39;, scale = alt.Scale(domain = (4,9))), y = alt.Y(\u0026#39;SepalWidth\u0026#39;, title = \u0026#39;Sepal Width\u0026#39;, scale = alt.Scale(domain = (1.5,4.5))), color = \u0026#39;Species\u0026#39;, shape = \u0026#39;Species\u0026#39;, tooltip = (\u0026#39;Species\u0026#39;)) \u003c!DOCTYPE html\u003e Notice that when you hover over the point, it lists the species value, because that is the one variable that we specified under the tooltip argument.\nWe can add as many different columns to the tooltip as we want.\n1 2 3 4 5 6 7 alt.Chart(iris, title=\u0026#34;Comparing Sepal Length to Sepal Width\u0026#34;).mark_point().encode( x = alt.X(\u0026#39;SepalLength\u0026#39;, title = \u0026#39;Sepal Length\u0026#39;, scale = alt.Scale(domain = (4,9))), y = alt.Y(\u0026#39;SepalWidth\u0026#39;, title = \u0026#39;Sepal Width\u0026#39;, scale = alt.Scale(domain = (1.5,4.5))), color = \u0026#39;Species\u0026#39;, shape = \u0026#39;Species\u0026#39;, tooltip = ([\u0026#39;Species\u0026#39;,\u0026#39;SepalLength\u0026#39;,\u0026#39;SepalWidth\u0026#39;,\u0026#39;PetalLength\u0026#39;,\u0026#39;PetalWidth\u0026#39;]) ) \u003c!DOCTYPE html\u003e Another feature we can add is the ability to make the graph interactive. This would allow the user to scroll or zoom.\n1 2 3 4 5 6 7 alt.Chart(iris, title=\u0026#34;Comparing Sepal Length to Sepal Width\u0026#34;).mark_point().encode( x = alt.X(\u0026#39;SepalLength\u0026#39;, title = \u0026#39;Sepal Length\u0026#39;, scale = alt.Scale(domain = (4,9))), y = alt.Y(\u0026#39;SepalWidth\u0026#39;, title = \u0026#39;Sepal Width\u0026#39;, scale = alt.Scale(domain = (1.5,4.5))), color = \u0026#39;Species\u0026#39;, shape = \u0026#39;Species\u0026#39;, tooltip = ([\u0026#39;Species\u0026#39;,\u0026#39;SepalLength\u0026#39;,\u0026#39;SepalWidth\u0026#39;,\u0026#39;PetalLength\u0026#39;,\u0026#39;PetalWidth\u0026#39;]) ).interactive() \u003c!DOCTYPE html\u003e Notice also there is a function called mark_circle() which is different than mark_point().\nWe can show the same graph as above, but with mark_circle() instead of mark_point().\n1 2 3 4 5 6 7 alt.Chart(iris, title=\u0026#34;Comparing Sepal Length to Sepal Width\u0026#34;).mark_circle().encode( x = alt.X(\u0026#39;SepalLength\u0026#39;, title = \u0026#39;Sepal Length\u0026#39;, scale = alt.Scale(domain = (4,9))), y = alt.Y(\u0026#39;SepalWidth\u0026#39;, title = \u0026#39;Sepal Width\u0026#39;, scale = alt.Scale(domain = (1.5,4.5))), color = \u0026#39;Species\u0026#39;, shape = \u0026#39;Species\u0026#39;, tooltip = ([\u0026#39;Species\u0026#39;,\u0026#39;SepalLength\u0026#39;,\u0026#39;SepalWidth\u0026#39;,\u0026#39;PetalLength\u0026#39;,\u0026#39;PetalWidth\u0026#39;]) ).interactive() \u003c!DOCTYPE html\u003e Let’s take a look at mark_line(), and let’s remove the shape argument.\n1 2 3 4 5 6 alt.Chart(iris, title=\u0026#34;Comparing Sepal Length to Sepal Width\u0026#34;).mark_line().encode( x = alt.X(\u0026#39;SepalLength\u0026#39;, title = \u0026#39;Sepal Length\u0026#39;, scale = alt.Scale(domain = (4,9))), y = alt.Y(\u0026#39;SepalWidth\u0026#39;, title = \u0026#39;Sepal Width\u0026#39;, scale = alt.Scale(domain = (1.5,4.5))), color = \u0026#39;Species\u0026#39;, tooltip = ([\u0026#39;Species\u0026#39;,\u0026#39;SepalLength\u0026#39;,\u0026#39;SepalWidth\u0026#39;,\u0026#39;PetalLength\u0026#39;,\u0026#39;PetalWidth\u0026#39;]) ).interactive() \u003c!DOCTYPE html\u003e We can overlay plots on top of each other. Let’s plot the scatterplot and lines together.\n1 2 3 4 5 6 7 8 9 10 11 12 13 line = alt.Chart(iris, title=\u0026#34;Comparing Sepal Length to Sepal Width\u0026#34;).mark_line().encode( x = alt.X(\u0026#39;SepalLength\u0026#39;, title = \u0026#39;Sepal Length\u0026#39;, scale = alt.Scale(domain = (4,9))), y = alt.Y(\u0026#39;SepalWidth\u0026#39;, title = \u0026#39;Sepal Width\u0026#39;, scale = alt.Scale(domain = (1.5,4.5))), color = \u0026#39;Species\u0026#39;, tooltip = ([\u0026#39;Species\u0026#39;,\u0026#39;SepalLength\u0026#39;,\u0026#39;SepalWidth\u0026#39;,\u0026#39;PetalLength\u0026#39;,\u0026#39;PetalWidth\u0026#39;]) ).interactive() point = alt.Chart(iris).mark_point().encode( x = \u0026#39;SepalLength\u0026#39;, y = \u0026#39;SepalWidth\u0026#39;, color = \u0026#39;Species\u0026#39;) line + point \u003c!DOCTYPE html\u003e We can also show multiple plots at once using the arguments we learned in part 3.\nRecall:\nor means horizontal and it is represented by | and means vertical and it is represented by \u0026amp; Try looking at them horizontally by typing line | point\nThen, try looking at them stacked vertically by typing line \u0026amp; point\nWe can also combine these features to design whatever layout you would like. First, let’s introduce a boxplot.\nIf we wanted to show a boxplot for the different petal lengths, we could do so like this:\n1 2 3 4 alt.Chart(iris, title = \u0026#39;Petal Lengths of Species\u0026#39;).mark_boxplot().encode( x = alt.X(\u0026#39;Species\u0026#39;, title = \u0026#39;Type of Species\u0026#39;), y = alt.Y(\u0026#39;PetalLength\u0026#39;, title = \u0026#39;Petal Length\u0026#39;) ) \u003c!DOCTYPE html\u003e To make it look nicer, we could add colour to each species, and then store it as a variable.\n1 2 3 4 5 6 box = alt.Chart(iris, title = \u0026#39;Petal Lengths of Species\u0026#39;).mark_boxplot().encode( x = alt.X(\u0026#39;Species\u0026#39;, title = \u0026#39;Type of Species\u0026#39;), y = alt.Y(\u0026#39;PetalLength\u0026#39;, title = \u0026#39;Petal Length\u0026#39;), color = \u0026#39;Species\u0026#39; ) box \u003c!DOCTYPE html\u003e If you want to show multiple plots in the same window, you can show 2 or more. Try writing line | point | box.\nYou can also mix and match operators. Using brackets will help to organize the layout. Try writing line | (point \u0026amp; box).\nMany different Altair charts can be created using mark_bar(), mark_line(), mark_point(), mark_rect(), and so many more!\nA complete list can be found here!\n","date":"2023-06-15T09:00:00-08:00","permalink":"http://localhost:5082/csc-data-blog/p/visualizing-with-altair-in-python/","title":"Visualizing with Altair in Python"},{"content":"R can be used as a generic calculator.\n1 1 + 1 1 ## [1] 2 1 2-3 1 ## [1] -1 1 6 / 2 1 ## [1] 3 1 3 * 4 1 ## [1] 12 Since R is so widely used, there are many people who contribute to continuously improving and developing it. Without anything added, R is a base version. It can do basic calculations, but it requires extra efforts to do more complicated things. People have created extras or add-ons to help create shortcuts for more complicated specific tasks or functions. These \u0026lsquo;add-ons\u0026rsquo; are called packages. Packages can be imported to use to help make things easier. For example, there is a large package in R called tidyverse that is very popular.\nFirst, packages must be installed, and then they have to be called. To install a package, use the command install.packages('package_name'). To call a package, use the command library(package_name). This is one example below:\n1 2 #install.packages(\u0026#39;matlib\u0026#39;) library(matlib) Once a package has been called, it does not need to be called again for the rest of this document. You can also import multiple packages within the same document. We will get into more useful packages later on!\nFor now, practice typing basic calculations into your R terminal to get used to the syntax, and see how easy it is to calculate things!\n","date":"2023-06-14T09:00:00-08:00","permalink":"http://localhost:5082/csc-data-blog/p/introduction-to-r/","title":"Introduction to R"},{"content":"Jupyter Notebook is a web-based interactive computing platform, and it can be used for many things, aside from Python. You can even use both R and Python within the same document.\nThink of Python (or R) as the language you are writing in, and Jupyter as the pen and pencil. This is how Python and Jupyter are used together.\nNote: There are many different IDEs (integrated development environment) for Python. To name a few, Programiz, Atom, Visual Studio Code, Spyder, and many more. If a Jupyter install is causing trouble, or if you do not like the interface and want something similar to R Studio, Spyder is a good option because it is a very familiar interface for R Studio users. Programiz is also great because it doesn\u0026rsquo;t require an install.\nSyzygy Syzygy (pronunciation up for debate) is a great way to use Jupyter without having to download it to your local computer. However, it does require your UBC login to use. It also can access your local files as well as UBC OneDrive files. To use, simply follow this link, click the sign on button in the top right, and use your UBC login to sign in. You will be able to creat files, view existing files, and enjoy all of the functionalities that Jupyter has to offer.\nDownloading Jupyter If you prefer to download Jupyter, it can be done with Anaconda. This video is a great resource explaining how to download both Jupyter and Anaconda, as well as how to get started using Jupyter.\nAlternatively, you can follow the instructions on this webpage to do the same install. Note that it has slightly different instructions for Windows and Mac users.\n","date":"2023-06-05T18:00:00-08:00","permalink":"http://localhost:5082/csc-data-blog/p/jupyter-notebook/","title":"Jupyter Notebook"},{"content":"CSV File Using the Pandas library, it is very easy to open a CSV file using Python. Simply import the package, and then use the line pd.read_csv(). See this example below:\n1 2 import pandas as pd df = pd.read_csv(\u0026#34;file.csv\u0026#34;) Note that this assumes the file you want to open follows the same filepath as your current directory. If you wanted to use a file in a different directory, simply use pd.read_csv(\u0026quot;/Users/name/rest_of_filepath/file.csv\u0026quot;) instead.\nAlternatively, if you were to use base Python, it gets a bit more complicated. Using the CSV library, it would have to be done by iterating through each row in the file. Printing each row would look like this:\n1 2 3 4 5 import csv with open(\u0026#34;file.csv\u0026#34;, \u0026#39;r\u0026#39;) as file: csvreader = csv.reader(file) for row in csvreader: print(row) 1 2 3 4 5 ## [\u0026#39;12\\t12\u0026#39;] ## [\u0026#39;34\\t1\u0026#39;] ## [\u0026#39;1\\t1\u0026#39;] ## [\u0026#39;1\\t1\u0026#39;] ## [\u0026#39;1\\t1\u0026#39;] Text File Similarly, for a text file, we can use Pandas to simplify the process. We can use the line pd.read_fwf() to read in a .txt file. For reference, FWF stands for fixed width lines which allows the lengths and features of the file to be specified as fixed values so that it can be read in to Python systematically. See this example below:\n1 2 import pandas as pd df = pd.read_fwf(\u0026#39;file.txt\u0026#39;) Alternatively, you can also iterate through each row similar to the csv format mentioned above, but we will not go through that. If you would like to try it as an exercise, simply use the second method above in the CSV section and change the file formatting to fit a .txt file.\nURL You can also access data on the internet without having to save a local copy. Take this website for example: Gapminder Data\nIf you follow the link, you will see that it is simply a CSV file with no other formatting or permissions to enter. If we wanted to read this into our Python script, we could do it like this:\n1 2 3 import pandas as pd url = \u0026#39;https://raw.githubusercontent.com/jstaf/gapminder/master/gapminder/gapminder.csv\u0026#39; df = pd.read_csv(url) There are many other file formats and ways to load files into Python. There are great resources online for any other file formats, but these examples above should get you started.\n","date":"2023-06-05T11:00:00-08:00","permalink":"http://localhost:5082/csc-data-blog/p/reading-data-in-python/","title":"Reading Data in Python"},{"content":" It is good practice to have descriptive but concise file names so one can easily find the code they are looking for. But as projects grow, the number of scripts can become quite large (unless you are one of those chaotic people that put everything in a single script\u0026hellip;). With UNIX computers, one can easily search for code in the a file explorer window, but this does not work with Windows. It is possible to search for files by name, but it is not possible to search for specific content in R scripts.\nThe findR package allows Windows users to search for specific strings of code within directories using a simple syntax:\n1 2 3 4 library(\u0026#39;findR\u0026#39;) findRscript(pattern = \u0026#39;a string of code\u0026#39;, path = \u0026#39;my-folder\u0026#39;, case.sensitive = TRUE) The pattern argument can be any code string, but note that single and double quotes need to be escaped by placing a \\ before them (i.e., \\' and \\\u0026quot;), while some special characters need to be preceeded by \\\\, such as +, (. Failing to escape and double-escape special characters will cause the function to miss the files or fail:\n1 findRscript(\u0026#39;library(\\\u0026#39;ctmm\\\u0026#39;)\u0026#39;, path = \u0026#39;H:/GitHub/env-var-review/analysis\u0026#39;) 1 ## No R scripts found! 1 findRscript(\u0026#39;library\\\\(\\\u0026#39;ctmm\\\u0026#39;\\\\)\u0026#39;, path = \u0026#39;H:/GitHub/env-var-review/analysis\u0026#39;) 1 ## No R scripts found! The package can also search for R Markdown files, PDFs, and text (.txt) files via the findRmd(), findPDF(), and findtxt() functions, respectively. Each of the functions can also copy the files that matched the pattern argument to a new folder (if copy = TRUE, but it is FALSE by default). You can decide which folder the files get copied to using the folder argument. Note that by default overwrite is set to TRUE, so any files present with the same name will be overwritten.\n","date":"2023-03-09T18:00:00-08:00","permalink":"http://localhost:5082/csc-data-blog/p/searching-for-files-with-the-findr-package/","title":"Searching for files with the `findR` package"},{"content":"\\usepackage{amsmath}\nThere are many different data structure types in R, each with varying levels of complexity and uses. The simplest data structure in R is a vector. Vectors are one-dimensional arrays (i.e., sets of values) of a single class (such as characters, numbers, dates, etc.), and they have a similar structure to the mathematical concept of vectors. For example, the vector $\\vec v$ with values 1, 4, 6, 2 would be:\n$$\\vec v = \\begin{bmatrix}1 \\\\ 4 \\\\ 6 \\\\ 2\\end{bmatrix}.$$In R we can create vectors using the c() function, as follows:\n1 2 v \u0026lt;- c(1, 4, 6, 2) v 1 ## [1] 1 4 6 2 (R prints vectors in a line rather than as columns to improve readability.)\nSince vectors can only contain elements of a common type, c() will force all elements to be of a single class. In the example below, I create a vector with the number 1, the letter \u0026ldquo;a\u0026rdquo;, today\u0026rsquo;s date, and the value TRUE (a boolean value), but c() coerces all values to be characters:\n1 c(1, \u0026#39;a\u0026#39;, Sys.Date(), TRUE) 1 ## [1] \u0026#34;1\u0026#34; \u0026#34;a\u0026#34; \u0026#34;20185\u0026#34; \u0026#34;TRUE\u0026#34; If we place two vectors (of the same class) side by side, we can create a matrix. Matrices are thus two-dimensional arrays\n1 2 # 2D arrays: matrices matrix(data = 1:8, nrow = 2) 1 2 3 ## [,1] [,2] [,3] [,4] ## [1,] 1 3 5 7 ## [2,] 2 4 6 8 1 matrix(data = 1:8, ncol = 2) 1 2 3 4 5 ## [,1] [,2] ## [1,] 1 5 ## [2,] 2 6 ## [3,] 3 7 ## [4,] 4 8 1 matrix(data = 1:8, ncol = 2, byrow = TRUE) 1 2 3 4 5 ## [,1] [,2] ## [1,] 1 2 ## [2,] 3 4 ## [3,] 5 6 ## [4,] 7 8 1 2 # matrix operations matrix(1:9, ncol = 3) 1 2 3 4 ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 1 matrix(1:9, ncol = 3) + 3 1 2 3 4 ## [,1] [,2] [,3] ## [1,] 4 7 10 ## [2,] 5 8 11 ## [3,] 6 9 12 1 matrix(1:9, ncol = 3) * 2 # NOT matrix multiplication 1 2 3 4 ## [,1] [,2] [,3] ## [1,] 2 8 14 ## [2,] 4 10 16 ## [3,] 6 12 18 1 matrix(1:9, ncol = 3) %*% 6:8 # matrix multiplication 1 2 3 4 ## [,1] ## [1,] 90 ## [2,] 111 ## [3,] 132 1 matrix(1:9, ncol = 3) %*% matrix(1:6, ncol = 2) 1 2 3 4 ## [,1] [,2] ## [1,] 30 66 ## [2,] 36 81 ## [3,] 42 96 If we want to create an object that has different data types, a matrix or vector won\u0026rsquo;t work because R will force all items to be of the same type.\n1 class(c(0, 2)) 1 ## [1] \u0026#34;numeric\u0026#34; 1 class(c(0, 2, \u0026#39;a\u0026#39;)) # coerces all items to text 1 ## [1] \u0026#34;character\u0026#34; Instead, we need to use a list.\n1 2 3 4 5 # grouping objects with different and types: lists l \u0026lt;- list(letters = c(\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;), numbers = 1:10, today = Sys.Date()) l 1 2 3 4 5 6 7 8 ## $letters ## [1] \u0026#34;a\u0026#34; \u0026#34;b\u0026#34; \u0026#34;c\u0026#34; ## ## $numbers ## [1] 1 2 3 4 5 6 7 8 9 10 ## ## $today ## [1] \u0026#34;2025-04-07\u0026#34; 1 l$letters 1 ## [1] \u0026#34;a\u0026#34; \u0026#34;b\u0026#34; \u0026#34;c\u0026#34; 1 l$today 1 ## [1] \u0026#34;2025-04-07\u0026#34; If we want a list that has a table-like structure (which will likely be the case for a lot of the data you use in R), we can use a data frame.\n1 2 data.frame(num = 1:10, abc = LETTERS[1:10]) 1 2 3 4 5 6 7 8 9 10 11 ## num abc ## 1 1 A ## 2 2 B ## 3 3 C ## 4 4 D ## 5 5 E ## 6 6 F ## 7 7 G ## 8 8 H ## 9 9 I ## 10 10 J 1 2 data.frame(num = 1:5, abc = LETTERS[1:10]) 1 2 3 4 5 6 7 8 9 10 11 ## num abc ## 1 1 A ## 2 2 B ## 3 3 C ## 4 4 D ## 5 5 E ## 6 1 F ## 7 2 G ## 8 3 H ## 9 4 I ## 10 5 J 1 2 data.frame(num = 1:10, abc = LETTERS[1:9]) 1 ## Error in data.frame(num = 1:10, abc = LETTERS[1:9]): arguments imply differing number of rows: 10, 9 The tidyverse set of packages provides a \u0026ldquo;fancy data frame\u0026rdquo; that does not recycle elements (unless they are a single value), and allows you to reference other columns you previously created:\n1 2 3 library(\u0026#39;tibble\u0026#39;) tibble(num = 1:10, abc = LETTERS[num]) 1 2 3 4 5 6 7 8 9 10 11 12 13 ## # A tibble: 10 × 2 ## num abc ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; ## 1 1 A ## 2 2 B ## 3 3 C ## 4 4 D ## 5 5 E ## 6 6 F ## 7 7 G ## 8 8 H ## 9 9 I ## 10 10 J 1 2 tibble(num = 1:5, abc = LETTERS[1:10]) 1 2 3 4 5 ## Error in `tibble()`: ## ! Tibble columns must have compatible sizes. ## • Size 5: Existing data. ## • Size 10: Column `abc`. ## ℹ Only values of size one are recycled. ","date":"2023-03-07T11:00:00-08:00","permalink":"http://localhost:5082/csc-data-blog/p/data-structures-in-r/","title":"Data Structures in R"},{"content":"Python can be used as a calculator 1 1 + 1 1 ## 2 1 2-3 1 ## -1 1 6 / 2 1 ## 3.0 1 3 * 4 1 ## 12 Since Python is so widely used, there are many people who contribute to continuously improving and developing it. Let\u0026rsquo;s imagine Python as a base version. It can do basic calculations, but it requires extra efforts to do more complicated things. People have created extras or add-ons to help create shortcuts for more complicated specific tasks or functions. These \u0026lsquo;add-ons\u0026rsquo; are called packages. Packages can be imported to use to help make things easier. For example, there is a Python package called math that helps with basic mathematical operations. Let\u0026rsquo;s look at an example here of calling the package using the import command.\n1 2 import math math.sqrt(9) 1 ## 3.0 Once it has been imported, it does not need to be re-imported for the rest of this document. You can also import multiple packages within the same document. We will get into more useful packages later on!\n1 math.log(100) 1 ## 4.605170185988092 For now, practice typing basic calculations into your Python terminal to get used to the syntax, and see how easy it is to calculate things!\n","date":"2023-02-15T09:00:00-08:00","permalink":"http://localhost:5082/csc-data-blog/p/introduction-to-python/","title":"Introduction to Python"},{"content":" The default color palettes ggplot2 provides are generally good enough for a quick check, but they are overused and do not have very high contrast:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 library(\u0026#39;ggplot2\u0026#39;) # for fancy figures library(\u0026#39;khroma\u0026#39;) # for fancy palettes library(\u0026#39;cowplot\u0026#39;) # for fancy multi-panel figures library(\u0026#39;colorspace\u0026#39;) # required by colorblindr library(\u0026#39;colorblindr\u0026#39;) # to simulate colorblind vision theme_set(theme_bw()) # change default ggplot theme p \u0026lt;- ggplot(mtcars, aes(disp, mpg, color = factor(carb))) + geom_point() + labs(x = \u0026#39;Displacement (cubic inches)\u0026#39;, y = \u0026#39;Miles per gallon (US)\u0026#39;, color = \u0026#39;Carburetors\u0026#39;) p Additionally, they are not colorblind-friendly:\n1 cvd_grid(p) While ggplot2 offers additional color palettes with more contrast, many of them are still not colorblind-friendly:\n1 p + scale_color_brewer(type = \u0026#39;qual\u0026#39;, palette = 6) 1 cvd_grid(p + scale_color_brewer(type = \u0026#39;qual\u0026#39;, palette = 6)) And while the viridis palette (from the viridisLite package and included in the ggplot2 package) can be a good option for continuous palettes, it can still be hard to distinguish between colors in qualitative palettes:\n1 p + scale_color_viridis_d() 1 cvd_grid(p + scale_color_viridis_d()) The khroma package provides multiple high-contrast, colorblind-friendly palettes for qualitative, diverging and sequential data, based on the work of Paul Tol (https://personal.sron.nl/~pault/) and Fabio Crameri (https://www.fabiocrameri.ch/).\n1 p + khroma::scale_color_bright() 1 cvd_grid(p + scale_color_bright()) However, it is best to also use different shapes (in addition to different colors), when possible, to ensure people are able to distinguish between each legend item (which can be difficult with many colors \u0026ndash; compare colors for 3 and 8 for deutan and protan versions, as well as the desaturated version). We can do this by specifying the shape argument. Note that we also need to change the name of the shape legend to ensure we get a single legend:\n1 2 3 4 5 6 p_sh \u0026lt;- ggplot(mtcars, aes(disp, mpg, color = factor(carb))) + geom_point(aes(shape = factor(carb))) + labs(x = \u0026#39;Displacement (cubic inches)\u0026#39;, y = \u0026#39;Miles per gallon (US)\u0026#39;, color = \u0026#39;Carburetors\u0026#39;, shape = \u0026#39;Carburetors\u0026#39;) p_sh 1 cvd_grid(p_sh) Continuous vs discrete khroma color palettes Unlike with the ggplot functions for color palettes, the khroma function names do not specify whether the function will produce a continuous or discrete color palette. To avoid confusion, you can type khroma::scale_color_ and press Tab to see what functions the package offers, and a helpful window should show up beside the function suggestion:\n(If you want to change your RStudio theme, see this tutorial.)\nInstalling the necessary packages To install colorblindr, you will first need to install the cowplot and colorspace packages:\n1 2 3 4 remotes::install_github(\u0026#34;wilkelab/cowplot\u0026#34;) install.packages(\u0026#34;colorspace\u0026#34;, repos = \u0026#34;http://R-Forge.R-project.org\u0026#34;) remotes::install_github(\u0026#34;clauswilke/colorblindr\u0026#34;) install.packages(\u0026#39;khroma\u0026#39;) To read the help files for any of the functions in the packages, use the ? function, e.g. ?scale_color_bright.\n","date":"2023-02-11T10:00:00-08:00","permalink":"http://localhost:5082/csc-data-blog/p/making-colorblind-friendly-figures/","title":"Making Colorblind-Friendly Figures"},{"content":"Color schemes are a matter of preference, but I think many of us can agree that RStudio\u0026rsquo;s default theme is quite bright and low-contrast. Fortunately, RStudio offers some alternative themes, which you can see by clicking on Tools \u0026gt; Global Options in the top ribbon menu in RStudio. You can then select Appearance and change the Editor theme.\nOf the default themes, the Cobalt theme is my preferred one, but I find some of the colors to be too similar (such as the white for general text and the light blue used for headings in R Markdown documents). You can find the custom theme I use on my GitHub page. To add use this theme, go to Tools \u0026gt; Global Options, then select Appearance, and click on Add. Finally, select the Black Rmd.rstheme file, and click on Open.\n","date":"2023-02-11T09:00:00-08:00","permalink":"http://localhost:5082/csc-data-blog/p/changing-the-rstudio-theme/","title":"Changing the RStudio Theme"},{"content":"R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nYou can embed an R code chunk like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 summary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 fit \u0026lt;- lm(dist ~ speed, data = cars) fit ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Coefficients: ## (Intercept) speed ## -17.579 3.932 Including Plots You can also embed plots. See Figure 1 for example:\n1 2 3 4 5 6 7 par(mar = c(0, 1, 0, 1)) pie( c(280, 60, 20), c(\u0026#39;Sky\u0026#39;, \u0026#39;Sunny side of pyramid\u0026#39;, \u0026#39;Shady side of pyramid\u0026#39;), col = c(\u0026#39;#0292D8\u0026#39;, \u0026#39;#F7EA39\u0026#39;, \u0026#39;#C4B632\u0026#39;), init.angle = -50, border = NA ) Figure 1: A fancy pie chart.\n","date":"2020-12-01T21:13:14-05:00","permalink":"http://localhost:5082/csc-data-blog/p/hello-r-markdown/","title":"Hello R Markdown"}]